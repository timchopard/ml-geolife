{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Simple baseline with Landsat and Bioclimatic Cubes + Sentinel images [0.31626]\n","\n","Following the three provided baselies with different modalities, we have provide a multimodal approch based on \"siamiese\" network with multiple inputs and simple shared \"decoder\". The links for the separated baselines are as follows:\n","\n","- [Baseline with Bioclimatic Cubes [0.25784]](https://www.kaggle.com/code/picekl/baseline-with-bioclimatic-cubes-0-25784)\n","- [Baseline with Landsat Cubes [0.26424]](https://www.kaggle.com/code/picekl/baseline-with-landsat-cubes-0-26424)\n","- [Baseline with Sentinel Images [0.23594]](https://www.kaggle.com/code/picekl/baseline-with-sentinel-images-0-23594)\n","\n","**Considering the significant extent for enhancing performance of this baseline, we encourage you to experiment with various techniques, architectures, losses, etc.**\n","\n","#### **Have Fun!**"]},{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2024-05-01T13:30:07.054038Z","iopub.status.busy":"2024-05-01T13:30:07.053659Z","iopub.status.idle":"2024-05-01T13:30:07.058148Z","shell.execute_reply":"2024-05-01T13:30:07.057269Z","shell.execute_reply.started":"2024-05-01T13:30:07.054008Z"}},"source":["# Data description\n","\n","## Landsat time series\n","\n","Satellite time series data includes over 20 years of Landsat satellite imagery extracted from [Ecodatacube](https://stac.ecodatacube.eu/).\n","The data was acquired through the Landsat satellite program and pre-processed by Ecodatacube to produce raster files scaled to the entire European continent and projected into a unique CRS.\n","\n","Since the original rasters require a high amount of disk space, we extracted the data points from each spectral band corresponding to all PA and PO locations (i.e., GPS coordinates) and aggregated them in (i) CSV files and (ii) data cubes as tensor objects. Each data point corresponds to the mean value of Landsat's observations at the given location for three months before the given time; e.g., the value of a time series element under column 2012_4 will represent the mean value for that element from October 2012 to December 2012.\n","\n","In this notebook, we will work with just the cubes. The cubes are structured as follows.\n","**Shape**: `(n_bands, n_quarters, n_years)` where:\n","- `n_bands` = 6 comprising [`red`, `green`, `blue`, `nir`, `swir1`, `swir2`]\n","- `n_quarters` = 4 \n","    - *Quarter 1*: December 2 of previous year until March 20 of current year (winter season proxy),\n","    - *Quarter 2*: March 21 until June 24 of current year (spring season proxy),\n","    - *Quarter 3*: June 25 until September 12 of current year (summer season proxy),\n","    - *Quarter 4*: September 13 until December 1 of current year (fall season proxy).\n","- `n_years` = 21 (ranging from 2000 to 2020)\n","\n","The datacubes can simply be loaded as tensors using PyTorch with the following command :\n","\n","```python\n","import torch\n","torch.load('path_to_file.pt')\n","```\n","\n","**References:**\n","- *Traceability (lineage): This dataset is a seasonally aggregated and gapfilled version of the Landsat GLAD analysis-ready data product presented by Potapov et al., 2020 ( https://doi.org/10.3390/rs12030426 ).*\n","- *Scientific methodology: The Landsat GLAD ARD dataset was aggregated and harmonized using the eumap python package (available at https://eumap.readthedocs.io/en/latest/ ). The full process of gapfilling and harmonization is described in detail in Witjes et al., 2022 (in review, preprint available at https://doi.org/10.21203/rs.3.rs-561383/v3 ).*\n","- *Ecodatacube.eu: Analysis-ready open environmental data cube for Europe (https://doi.org/10.21203/rs.3.rs-2277090/v3).*\n","\n","\n","## Bioclimatic time series\n","\n","The Bioclimatic Cubes are created from **four** monthly GeoTIFF CHELSA (https://chelsa-climate.org/timeseries/) time series climatic rasters with a resolution of 30 arc seconds, i.e. approximately 1km. The four variables are the precipitation (pr), maximum- (taxmax), minimum- (tasmin), and mean (tax) daily temperatures per month from January 2000 to June 2019. We provide the data in three forms: (i) raw rasters (GeoTiff images), (ii) CSV file with pre-extracted values for each location, i.e., surveyId, and (iii) data cubes as tensor object (.pt).\n","\n","In this notebook, we will work with just the cubes. The cubes are structured as follows.\n","**Shape**: `(n_year, n_month, n_bio)` where:\n","- `n_year` = 19 (ranging from 2000 to 2018)\n","- `n_month` = 12 (ranging from January 01 to December 12)\n","- `n_bio` = 4 comprising [`pr` (precipitation), `tas` (mean daily air temperature), `tasmin`, `tasmax`]\n","\n","The datacubes can simply be loaded as tensors using PyTorch with the following command :\n","\n","```python\n","import torch\n","torch.load('path_to_file.pt')\n","```\n","\n","**References:**\n","- *Karger, D.N., Conrad, O., Böhner, J., Kawohl, T., Kreft, H., Soria-Auza, R.W., Zimmermann, N.E., Linder, P., Kessler, M. (2017): Climatologies at high resolution for the Earth land surface areas. Scientific Data. 4 170122. https://doi.org/10.1038/sdata.2017.122*\n","\n","- *Karger D.N., Conrad, O., Böhner, J., Kawohl, T., Kreft, H., Soria-Auza, R.W., Zimmermann, N.E, Linder, H.P., Kessler, M. Data from: Climatologies at high resolution for the earth’s land surface areas. Dryad Digital Repository. http://dx.doi.org/doi:10.5061/dryad.kd1d4*\n","\n","\n","## Sentinel Image Patches\n","\n","The Sentinel Image data was acquired through the Sentinel2 satellite program and pre-processed by [Ecodatacube](https://stac.ecodatacube.eu/) to produce raster files scaled to the entire European continent and projected into a unique CRS. We filtered the data in order to pick patches from each spectral band corresponding to a location ((lon, lat) GPS coordinates) and a date matching that of our occurrences', and split them into JPEG files (RGB in 3-channels .jpeg files and NIR in single-channel .jpeg files) with a 128x128 resolution. The images were converted from sentinel uint15 to uint8 by clipping data pixel values over 10000 and applying a gamma correction of 2.5.\n","\n","The data can simply be loaded using the following method:\n","\n","```python\n","def construct_patch_path(output_path, survey_id):\n","    \"\"\"Construct the patch file path based on survey_id as './CD/AB/XXXXABCD.jpeg'\"\"\"\n","    path = output_path\n","    for d in (str(survey_id)[-2:], str(survey_id)[-4:-2]):\n","        path = os.path.join(path, d)\n","\n","    path = os.path.join(path, f\"{survey_id}.jpeg\")\n","\n","    return path\n","```\n","\n","**References:**\n","- *Traceability (lineage): The dataset was produced entirely by mosaicking and seasonally aggregating imagery from the Sentinel-2 Level-2A product (https://sentinels.copernicus.eu/web/sentinel/user-guides/sentinel-2-msi/product-types/level-2a)*\n","- *Ecodatacube.eu: Analysis-ready open environmental data cube for Europe (https://doi.org/10.21203/rs.3.rs-2277090/v3)*"]},{"cell_type":"code","execution_count":9,"metadata":{"ExecuteTime":{"end_time":"2024-04-30T21:25:07.29831Z","start_time":"2024-04-30T21:25:05.354584Z"},"execution":{"iopub.execute_input":"2024-05-23T12:02:45.113952Z","iopub.status.busy":"2024-05-23T12:02:45.113590Z","iopub.status.idle":"2024-05-23T12:02:53.937870Z","shell.execute_reply":"2024-05-23T12:02:53.936751Z","shell.execute_reply.started":"2024-05-23T12:02:45.113924Z"},"tags":[],"trusted":true},"outputs":[],"source":["import os\n","import torch\n","import tqdm\n","import numpy as np\n","import pandas as pd\n","import torchvision.models as models\n","import torchvision.transforms as transforms\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from torch.optim.lr_scheduler import CosineAnnealingLR\n","#from sklearn.metrics import precision_recall_fscore_support"]},{"cell_type":"markdown","metadata":{},"source":["## Prepare custom dataset loader\n","\n","We have to slightly update the Dataset to provide the relevant data in the appropriate format."]},{"cell_type":"code","execution_count":10,"metadata":{"ExecuteTime":{"end_time":"2024-04-30T21:25:32.627928Z","start_time":"2024-04-30T21:25:32.612131Z"},"collapsed":false,"execution":{"iopub.execute_input":"2024-05-23T12:02:53.940623Z","iopub.status.busy":"2024-05-23T12:02:53.940058Z","iopub.status.idle":"2024-05-23T12:02:53.969365Z","shell.execute_reply":"2024-05-23T12:02:53.967980Z","shell.execute_reply.started":"2024-05-23T12:02:53.940591Z"},"jupyter":{"outputs_hidden":false},"tags":[],"trusted":true},"outputs":[],"source":["from PIL import Image\n","\n","def construct_patch_path(data_path, survey_id):\n","    \"\"\"Construct the patch file path based on plot_id as './CD/AB/XXXXABCD.jpeg'\"\"\"\n","    path = data_path\n","    for d in (str(survey_id)[-2:], str(survey_id)[-4:-2]):\n","        path = os.path.join(path, d)\n","\n","    path = os.path.join(path, f\"{survey_id}.jpeg\")\n","\n","    return path\n","\n","class TrainDataset(Dataset):\n","    def __init__(self, bioclim_data_dir, landsat_data_dir, sentinel_data_dir, metadata, transform=None):\n","        self.transform = transform\n","        self.sentinel_transform = transform = transforms.Compose([\n","            transforms.ToTensor(),\n","            transforms.Normalize(mean=(0.485, 0.456, 0.406, 0.5), std=(0.229, 0.224, 0.225, 0.25)),\n","        ])\n","\n","        self.bioclim_data_dir = bioclim_data_dir\n","        self.landsat_data_dir = landsat_data_dir\n","        self.sentinel_data_dir = sentinel_data_dir\n","        self.metadata = metadata\n","        self.metadata = self.metadata.dropna(subset=\"speciesId\").reset_index(drop=True)\n","        self.metadata['speciesId'] = self.metadata['speciesId'].astype(int)\n","        self.label_dict = self.metadata.groupby('surveyId')['speciesId'].apply(list).to_dict()\n","\n","        self.metadata = self.metadata.drop_duplicates(subset=\"surveyId\").reset_index(drop=True)\n","\n","    def __len__(self):\n","        return len(self.metadata)\n","\n","    def __getitem__(self, idx):\n","\n","        survey_id = self.metadata.surveyId[idx]\n","\n","        landsat_sample = torch.nan_to_num(torch.load(os.path.join(self.landsat_data_dir, f\"GLC24-PA-train-landsat-time-series_{survey_id}_cube.pt\")))\n","        bioclim_sample = torch.nan_to_num(torch.load(os.path.join(self.bioclim_data_dir, f\"GLC24-PA-train-bioclimatic_monthly_{survey_id}_cube.pt\")))\n","\n","        rgb_sample = np.array(Image.open(construct_patch_path(self.sentinel_data_dir, survey_id)))\n","        nir_sample = np.array(Image.open(construct_patch_path(self.sentinel_data_dir.replace(\"rgb\", \"nir\").replace(\"RGB\", \"NIR\"), survey_id)))\n","        sentinel_sample = np.concatenate((rgb_sample, nir_sample[...,None]), axis=2)\n","\n","        species_ids = self.label_dict.get(survey_id, [])  # Get list of species IDs for the survey ID\n","        label = torch.zeros(num_classes)  # Initialize label tensor\n","        for species_id in species_ids:\n","            label_id = species_id\n","            label[label_id] = 1  # Set the corresponding class index to 1 for each species\n","\n","        if isinstance(landsat_sample, torch.Tensor):\n","            landsat_sample = landsat_sample.permute(1, 2, 0)  # Change tensor shape from (C, H, W) to (H, W, C)\n","            landsat_sample = landsat_sample.numpy()  # Convert tensor to numpy array\n","\n","        if isinstance(bioclim_sample, torch.Tensor):\n","            bioclim_sample = bioclim_sample.permute(1, 2, 0)  # Change tensor shape from (C, H, W) to (H, W, C)\n","            bioclim_sample = bioclim_sample.numpy()  # Convert tensor to numpy array\n","\n","        if self.transform:\n","            landsat_sample = self.transform(landsat_sample)\n","            bioclim_sample = self.transform(bioclim_sample)\n","            sentinel_sample = self.sentinel_transform(sentinel_sample)\n","\n","        return landsat_sample, bioclim_sample, sentinel_sample, label, survey_id\n","\n","class TestDataset(TrainDataset):\n","    def __init__(self, bioclim_data_dir, landsat_data_dir, sentinel_data_dir, metadata, transform=None):\n","        self.transform = transform\n","        self.sentinel_transform = transform = transforms.Compose([\n","            transforms.ToTensor(),\n","            transforms.Normalize(mean=(0.485, 0.456, 0.406, 0.5), std=(0.229, 0.224, 0.225, 0.25)),\n","        ])\n","\n","        self.bioclim_data_dir = bioclim_data_dir\n","        self.landsat_data_dir = landsat_data_dir\n","        self.sentinel_data_dir = sentinel_data_dir\n","        self.metadata = metadata\n","\n","    def __getitem__(self, idx):\n","\n","        survey_id = self.metadata.surveyId[idx]\n","        landsat_sample = torch.nan_to_num(torch.load(os.path.join(self.landsat_data_dir, f\"GLC24-PA-test-landsat_time_series_{survey_id}_cube.pt\")))\n","        bioclim_sample = torch.nan_to_num(torch.load(os.path.join(self.bioclim_data_dir, f\"GLC24-PA-test-bioclimatic_monthly_{survey_id}_cube.pt\")))\n","\n","        rgb_sample = np.array(Image.open(construct_patch_path(self.sentinel_data_dir, survey_id)))\n","        nir_sample = np.array(Image.open(construct_patch_path(self.sentinel_data_dir.replace(\"rgb\", \"nir\").replace(\"RGB\", \"NIR\"), survey_id)))\n","        sentinel_sample = np.concatenate((rgb_sample, nir_sample[...,None]), axis=2)\n","\n","        if isinstance(landsat_sample, torch.Tensor):\n","            landsat_sample = landsat_sample.permute(1, 2, 0)  # Change tensor shape from (C, H, W) to (H, W, C)\n","            landsat_sample = landsat_sample.numpy()  # Convert tensor to numpy array\n","\n","        if isinstance(bioclim_sample, torch.Tensor):\n","            bioclim_sample = bioclim_sample.permute(1, 2, 0)  # Change tensor shape from (C, H, W) to (H, W, C)\n","            bioclim_sample = bioclim_sample.numpy()  # Convert tensor to numpy array\n","\n","        if self.transform:\n","            landsat_sample = self.transform(landsat_sample)\n","            bioclim_sample = self.transform(bioclim_sample)\n","            sentinel_sample = self.sentinel_transform(sentinel_sample)\n","\n","        return landsat_sample, bioclim_sample, sentinel_sample, survey_id"]},{"cell_type":"markdown","metadata":{},"source":["### Load metadata and prepare data loaders"]},{"cell_type":"code","execution_count":11,"metadata":{"ExecuteTime":{"end_time":"2024-04-30T21:25:34.532017Z","start_time":"2024-04-30T21:25:32.615562Z"},"collapsed":false,"execution":{"iopub.execute_input":"2024-05-23T12:02:53.972163Z","iopub.status.busy":"2024-05-23T12:02:53.971145Z","iopub.status.idle":"2024-05-23T12:03:00.453454Z","shell.execute_reply":"2024-05-23T12:03:00.452223Z","shell.execute_reply.started":"2024-05-23T12:02:53.972121Z"},"jupyter":{"outputs_hidden":false},"tags":[],"trusted":true},"outputs":[{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[11], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m train_metadata_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/geolifeclef-2024/GLC24_PA_metadata_train.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     13\u001b[0m train_metadata \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(train_metadata_path)\n\u001b[0;32m---> 14\u001b[0m dataset_alpine \u001b[38;5;241m=\u001b[39m \u001b[43mTrainDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_bioclim_data_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_landsat_data_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_sentinel_data_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_metadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(dataset_alpine, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Load Test metadata\u001b[39;00m\n","Cell \u001b[0;32mIn[10], line 27\u001b[0m, in \u001b[0;36mTrainDataset.__init__\u001b[0;34m(self, bioclim_data_dir, landsat_data_dir, sentinel_data_dir, metadata, transform)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mdropna(subset\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspeciesId\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspeciesId\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspeciesId\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msurveyId\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mspeciesId\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto_dict()\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mdrop_duplicates(subset\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msurveyId\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n","File \u001b[0;32m~/miniconda3/envs/pt/lib/python3.11/site-packages/pandas/core/groupby/generic.py:230\u001b[0m, in \u001b[0;36mSeriesGroupBy.apply\u001b[0;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;129m@Appender\u001b[39m(\n\u001b[1;32m    225\u001b[0m     _apply_docs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemplate\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    226\u001b[0m         \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseries\u001b[39m\u001b[38;5;124m\"\u001b[39m, examples\u001b[38;5;241m=\u001b[39m_apply_docs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseries_examples\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    227\u001b[0m     )\n\u001b[1;32m    228\u001b[0m )\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Series:\n\u001b[0;32m--> 230\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/miniconda3/envs/pt/lib/python3.11/site-packages/pandas/core/groupby/groupby.py:1824\u001b[0m, in \u001b[0;36mGroupBy.apply\u001b[0;34m(self, func, include_groups, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1822\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1823\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1824\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_python_apply_general\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selected_obj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1825\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1826\u001b[0m             \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj, Series)\n\u001b[1;32m   1827\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1828\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selected_obj\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obj_with_exclusions\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m   1829\u001b[0m         ):\n\u001b[1;32m   1830\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1831\u001b[0m                 message\u001b[38;5;241m=\u001b[39m_apply_groupings_depr\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1832\u001b[0m                     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1835\u001b[0m                 stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m   1836\u001b[0m             )\n","File \u001b[0;32m~/miniconda3/envs/pt/lib/python3.11/site-packages/pandas/core/groupby/groupby.py:1885\u001b[0m, in \u001b[0;36mGroupBy._python_apply_general\u001b[0;34m(self, f, data, not_indexed_same, is_transform, is_agg)\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[1;32m   1851\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_python_apply_general\u001b[39m(\n\u001b[1;32m   1852\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1857\u001b[0m     is_agg: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1858\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NDFrameT:\n\u001b[1;32m   1859\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1860\u001b[0m \u001b[38;5;124;03m    Apply function f in python space\u001b[39;00m\n\u001b[1;32m   1861\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1883\u001b[0m \u001b[38;5;124;03m        data after applying f\u001b[39;00m\n\u001b[1;32m   1884\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1885\u001b[0m     values, mutated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_grouper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_groupwise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1886\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m not_indexed_same \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1887\u001b[0m         not_indexed_same \u001b[38;5;241m=\u001b[39m mutated\n","File \u001b[0;32m~/miniconda3/envs/pt/lib/python3.11/site-packages/pandas/core/groupby/ops.py:919\u001b[0m, in \u001b[0;36mBaseGrouper.apply_groupwise\u001b[0;34m(self, f, data, axis)\u001b[0m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;66;03m# group might be modified\u001b[39;00m\n\u001b[1;32m    918\u001b[0m group_axes \u001b[38;5;241m=\u001b[39m group\u001b[38;5;241m.\u001b[39maxes\n\u001b[0;32m--> 919\u001b[0m res \u001b[38;5;241m=\u001b[39m f(group)\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mutated \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_indexed_like(res, group_axes, axis):\n\u001b[1;32m    921\u001b[0m     mutated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n","File \u001b[0;32m~/miniconda3/envs/pt/lib/python3.11/site-packages/pandas/core/base.py:862\u001b[0m, in \u001b[0;36mIndexOpsMixin.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28miter\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values)\n\u001b[1;32m    861\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values\u001b[38;5;241m.\u001b[39mitem, \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values\u001b[38;5;241m.\u001b[39msize))\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# Dataset and DataLoader\n","batch_size = 64\n","transform = transforms.Compose([\n","    transforms.ToTensor()\n","])\n","\n","# Load Training metadata\n","train_landsat_data_path = \"data/geolifeclef-2024/TimeSeries-Cubes/TimeSeries-Cubes/GLC24-PA-train-landsat_time_series/\"\n","train_bioclim_data_path = \"data/geolifeclef-2024/TimeSeries-Cubes/TimeSeries-Cubes/GLC24-PA-train-bioclimatic_monthly/\"\n","train_sentinel_data_path=\"data/geolifeclef-2024/PA_Train_SatellitePatches_RGB/pa_train_patches_rgb/\"\n","train_metadata_path = \"data/geolifeclef-2024/GLC24_PA_metadata_train.csv\"\n","\n","train_metadata = pd.read_csv(train_metadata_path)\n","dataset_alpine = TrainDataset(train_bioclim_data_path, train_landsat_data_path, train_sentinel_data_path, train_metadata, transform=transform)\n","train_loader = DataLoader(dataset_alpine, batch_size=batch_size, shuffle=True, num_workers=4)\n","\n","# Load Test metadata\n","test_landsat_data_path = \"data/geolifeclef-2024/TimeSeries-Cubes/TimeSeries-Cubes/GLC24-PA-test-landsat_time_series/\"\n","test_bioclim_data_path = \"data/geolifeclef-2024/TimeSeries-Cubes/TimeSeries-Cubes/GLC24-PA-test-bioclimatic_monthly/\"\n","test_sentinel_data_path = \"data/geolifeclef-2024/PA_Test_SatellitePatches_RGB/pa_test_patches_rgb/\"\n","test_metadata_path = \"data/geolifeclef-2024/GLC24_PA_metadata_test.csv\"\n","\n","test_metadata = pd.read_csv(test_metadata_path)\n","test_dataset = TestDataset(test_bioclim_data_path, test_landsat_data_path, test_sentinel_data_path, test_metadata, transform=transform)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)"]},{"cell_type":"markdown","metadata":{},"source":["## Define and initialize a Multimodal Model\n","\n","To process multiple inputs with different modalities and formats we use so-call siamiese approach where each modality is processed with different backbone (i.e., encoder). Data encoded into a 1d vector are concatenated and classified with a simple fully connected neural network. Short recap from previous notebooks.\n","- The Landsat cubes have a shape of [6,4,21] (BANDs, QUARTERs, and YEARs).\n","- The Bioclimatic cubes have a shape of [4,19,12] (RASTER-TYPE, YEAR, and MONTH)\n","- The Sentinel Image Patches have a shape od [128, 128, 4] (R, G, B, NIR)"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2024-04-30T21:25:31.014067Z","start_time":"2024-04-30T21:25:31.01006Z"},"collapsed":false,"execution":{"iopub.execute_input":"2024-05-23T12:03:00.456493Z","iopub.status.busy":"2024-05-23T12:03:00.456108Z","iopub.status.idle":"2024-05-23T12:03:00.473464Z","shell.execute_reply":"2024-05-23T12:03:00.472241Z","shell.execute_reply.started":"2024-05-23T12:03:00.456463Z"},"jupyter":{"outputs_hidden":false},"tags":[],"trusted":true},"outputs":[],"source":["import torch.nn.functional as F\n","\n","class MultimodalEnsemble(nn.Module):\n","    def __init__(self, num_classes):\n","        super(MultimodalEnsemble, self).__init__()\n","\n","        self.landsat_norm = nn.LayerNorm([6,4,21])\n","        self.landsat_model = models.resnet34(weights=None)\n","        # Modify the first convolutional layer to accept 6 channels instead of 3\n","        self.landsat_model.conv1 = nn.Conv2d(6, 64, kernel_size=3, stride=1, padding=1, bias=False)\n","        self.landsat_model.maxpool = nn.Identity()\n","\n","        self.bioclim_norm = nn.LayerNorm([4,19,12])\n","        self.bioclim_model = models.resnet34(weights=None)\n","        # Modify the first convolutional layer to accept 4 channels instead of 3\n","        self.bioclim_model.conv1 = nn.Conv2d(4, 64, kernel_size=3, stride=1, padding=1, bias=False)\n","        self.bioclim_model.maxpool = nn.Identity()\n","\n","        self.sentinel_model = models.swin_t(weights=\"IMAGENET1K_V1\")\n","        # Modify the first layer to accept 4 channels instead of 3\n","        self.sentinel_model.features[0][0] = nn.Conv2d(4, 96, kernel_size=(4, 4), stride=(4, 4))\n","        self.sentinel_model.head = nn.Identity()\n","\n","        self.ln1 = nn.LayerNorm(1000)\n","        self.ln2 = nn.LayerNorm(1000)\n","        self.fc1 = nn.Linear(2768, 4096)\n","        self.fc2 = nn.Linear(4096, num_classes)\n","\n","        self.dropout = nn.Dropout(p=0.3)\n","\n","    def forward(self, x, y, z):\n","\n","        x = self.landsat_norm(x)\n","        x = self.landsat_model(x)\n","        x = self.ln1(x)\n","\n","        y = self.bioclim_norm(y)\n","        y = self.bioclim_model(y)\n","        y = self.ln2(y)\n","\n","        z = self.sentinel_model(z)\n","\n","        xyz = torch.cat((x, y, z), dim=1)\n","        xyz = self.fc1(xyz)\n","        xyz = self.dropout(xyz)\n","        out = self.fc2(xyz)\n","        return out"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T12:03:00.475068Z","iopub.status.busy":"2024-05-23T12:03:00.474722Z","iopub.status.idle":"2024-05-23T12:03:00.513560Z","shell.execute_reply":"2024-05-23T12:03:00.512665Z","shell.execute_reply.started":"2024-05-23T12:03:00.475031Z"},"tags":[],"trusted":true},"outputs":[],"source":["def set_seed(seed):\n","    # Set seed for Python's built-in random number generator\n","    torch.manual_seed(seed)\n","    # Set seed for numpy\n","    np.random.seed(seed)\n","    # Set seed for CUDA if available\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed_all(seed)\n","        # Set cuDNN's random number generator seed for deterministic behavior\n","        torch.backends.cudnn.deterministic = True\n","        torch.backends.cudnn.benchmark = False\n","\n","set_seed(69)"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2024-04-30T21:25:31.611823Z","start_time":"2024-04-30T21:25:31.607373Z"},"collapsed":false,"execution":{"iopub.execute_input":"2024-05-23T12:03:00.514998Z","iopub.status.busy":"2024-05-23T12:03:00.514704Z","iopub.status.idle":"2024-05-23T12:03:03.763004Z","shell.execute_reply":"2024-05-23T12:03:03.761571Z","shell.execute_reply.started":"2024-05-23T12:03:00.514973Z"},"jupyter":{"outputs_hidden":false},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["DEVICE = CUDA\n"]}],"source":["# Check if cuda is available\n","device = torch.device(\"cpu\")\n","\n","if torch.cuda.is_available():\n","    device = torch.device(\"cuda\")\n","    print(\"DEVICE = CUDA\")\n","\n","num_classes = 11255 # Number of all unique classes within the PO and PA data.\n","model = MultimodalEnsemble(num_classes).to(device)"]},{"cell_type":"markdown","metadata":{},"source":["## Training Loop\n","\n","Nothing special, just a standard Pytorch training loop."]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2024-04-30T21:25:32.181927Z","start_time":"2024-04-30T21:25:32.177073Z"},"collapsed":false,"execution":{"iopub.execute_input":"2024-05-23T12:03:03.764842Z","iopub.status.busy":"2024-05-23T12:03:03.764519Z","iopub.status.idle":"2024-05-23T12:03:03.774203Z","shell.execute_reply":"2024-05-23T12:03:03.772766Z","shell.execute_reply.started":"2024-05-23T12:03:03.764814Z"},"jupyter":{"outputs_hidden":false},"tags":[],"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/startung/miniconda3/envs/pt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n","  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"]}],"source":["# Hyperparameters\n","learning_rate = 0.00025\n","num_epochs = 50\n","positive_weigh_factor = 1.0\n","\n","optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n","scheduler = CosineAnnealingLR(optimizer, T_max=25, verbose=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"start_time":"2024-04-30T21:25:34.536634Z"},"collapsed":false,"execution":{"iopub.execute_input":"2024-05-23T12:03:03.776583Z","iopub.status.busy":"2024-05-23T12:03:03.775716Z"},"is_executing":true,"jupyter":{"outputs_hidden":false},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Training for 50 epochs started.\n","Epoch 1/50, Batch 0/1391, Loss: 0.708842396736145\n","Epoch 1/50, Batch 278/1391, Loss: 0.005518469959497452\n","Epoch 1/50, Batch 556/1391, Loss: 0.005290952045470476\n","Epoch 1/50, Batch 834/1391, Loss: 0.0055296774953603745\n","Epoch 1/50, Batch 1112/1391, Loss: 0.004871605895459652\n"]},{"name":"stderr","output_type":"stream","text":["/home/startung/miniconda3/envs/pt/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at /opt/conda/conda-bld/pytorch_1712608853085/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n","  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/50, Batch 1390/1391, Loss: 0.005387415643781424\n","Scheduler: {'T_max': 25, 'eta_min': 0, 'base_lrs': [0.00025], 'last_epoch': 1, 'verbose': True, '_step_count': 2, '_get_lr_called_within_step': False, '_last_lr': [0.00024901433766430975]}\n","Epoch 2/50, Batch 0/1391, Loss: 0.0051876818761229515\n","Epoch 2/50, Batch 278/1391, Loss: 0.004460916854441166\n","Epoch 2/50, Batch 556/1391, Loss: 0.005299861077219248\n","Epoch 2/50, Batch 834/1391, Loss: 0.004453381057828665\n","Epoch 2/50, Batch 1112/1391, Loss: 0.004165438935160637\n","Epoch 2/50, Batch 1390/1391, Loss: 0.004520082380622625\n","Scheduler: {'T_max': 25, 'eta_min': 0, 'base_lrs': [0.00025], 'last_epoch': 2, 'verbose': True, '_step_count': 3, '_get_lr_called_within_step': False, '_last_lr': [0.0002460728951410789]}\n","Epoch 3/50, Batch 0/1391, Loss: 0.005325914826244116\n","Epoch 3/50, Batch 278/1391, Loss: 0.004192039370536804\n","Epoch 3/50, Batch 556/1391, Loss: 0.004364110063761473\n","Epoch 3/50, Batch 834/1391, Loss: 0.004323952365666628\n","Epoch 3/50, Batch 1112/1391, Loss: 0.0040291352197527885\n","Epoch 3/50, Batch 1390/1391, Loss: 0.0045546917244791985\n","Scheduler: {'T_max': 25, 'eta_min': 0, 'base_lrs': [0.00025], 'last_epoch': 3, 'verbose': True, '_step_count': 4, '_get_lr_called_within_step': False, '_last_lr': [0.00024122206073603144]}\n","Epoch 4/50, Batch 0/1391, Loss: 0.004218364600092173\n","Epoch 4/50, Batch 278/1391, Loss: 0.004322038032114506\n","Epoch 4/50, Batch 556/1391, Loss: 0.0048062545247375965\n","Epoch 4/50, Batch 834/1391, Loss: 0.003768197726458311\n","Epoch 4/50, Batch 1112/1391, Loss: 0.003864384489133954\n","Epoch 4/50, Batch 1390/1391, Loss: 0.0038205510936677456\n","Scheduler: {'T_max': 25, 'eta_min': 0, 'base_lrs': [0.00025], 'last_epoch': 4, 'verbose': True, '_step_count': 5, '_get_lr_called_within_step': False, '_last_lr': [0.00023453833500548295]}\n","Epoch 5/50, Batch 0/1391, Loss: 0.004065211396664381\n","Epoch 5/50, Batch 278/1391, Loss: 0.003528660861775279\n","Epoch 5/50, Batch 556/1391, Loss: 0.003995321691036224\n","Epoch 5/50, Batch 834/1391, Loss: 0.004321588668972254\n","Epoch 5/50, Batch 1112/1391, Loss: 0.0036587214563041925\n","Epoch 5/50, Batch 1390/1391, Loss: 0.004418111871927977\n","Scheduler: {'T_max': 25, 'eta_min': 0, 'base_lrs': [0.00025], 'last_epoch': 5, 'verbose': True, '_step_count': 6, '_get_lr_called_within_step': False, '_last_lr': [0.00022612712429686844]}\n","Epoch 6/50, Batch 0/1391, Loss: 0.004081046208739281\n","Epoch 6/50, Batch 278/1391, Loss: 0.004721371922641993\n","Epoch 6/50, Batch 556/1391, Loss: 0.003975949250161648\n","Epoch 6/50, Batch 834/1391, Loss: 0.004070750437676907\n","Epoch 6/50, Batch 1112/1391, Loss: 0.003930147271603346\n","Epoch 6/50, Batch 1390/1391, Loss: 0.004826302640140057\n","Scheduler: {'T_max': 25, 'eta_min': 0, 'base_lrs': [0.00025], 'last_epoch': 6, 'verbose': True, '_step_count': 7, '_get_lr_called_within_step': False, '_last_lr': [0.00021612107842767644]}\n","Epoch 7/50, Batch 0/1391, Loss: 0.004851295612752438\n","Epoch 7/50, Batch 278/1391, Loss: 0.0047151511535048485\n","Epoch 7/50, Batch 556/1391, Loss: 0.0037179067730903625\n","Epoch 7/50, Batch 834/1391, Loss: 0.0036753625608980656\n","Epoch 7/50, Batch 1112/1391, Loss: 0.004048563539981842\n","Epoch 7/50, Batch 1390/1391, Loss: 0.004428558517247438\n","Scheduler: {'T_max': 25, 'eta_min': 0, 'base_lrs': [0.00025], 'last_epoch': 7, 'verbose': True, '_step_count': 8, '_get_lr_called_within_step': False, '_last_lr': [0.0002046779987185862]}\n","Epoch 8/50, Batch 0/1391, Loss: 0.0038517042994499207\n","Epoch 8/50, Batch 278/1391, Loss: 0.0037444555200636387\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[8], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[1;32m     20\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 21\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_idx \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m278\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Batch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_loader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m~/miniconda3/envs/pt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:75\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m instance\u001b[38;5;241m.\u001b[39m_step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     74\u001b[0m wrapped \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance, \u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m---> 75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/miniconda3/envs/pt/lib/python3.11/site-packages/torch/optim/optimizer.py:391\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    387\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    388\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    389\u001b[0m             )\n\u001b[0;32m--> 391\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    394\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n","File \u001b[0;32m~/miniconda3/envs/pt/lib/python3.11/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n","File \u001b[0;32m~/miniconda3/envs/pt/lib/python3.11/site-packages/torch/optim/adamw.py:188\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    175\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    177\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    178\u001b[0m         group,\n\u001b[1;32m    179\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    185\u001b[0m         state_steps,\n\u001b[1;32m    186\u001b[0m     )\n\u001b[0;32m--> 188\u001b[0m     \u001b[43madamw\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n","File \u001b[0;32m~/miniconda3/envs/pt/lib/python3.11/site-packages/torch/optim/adamw.py:340\u001b[0m, in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    338\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adamw\n\u001b[0;32m--> 340\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/miniconda3/envs/pt/lib/python3.11/site-packages/torch/optim/adamw.py:613\u001b[0m, in \u001b[0;36m_multi_tensor_adamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, has_complex)\u001b[0m\n\u001b[1;32m    611\u001b[0m torch\u001b[38;5;241m.\u001b[39m_foreach_div_(exp_avg_sq_sqrt, bias_correction2_sqrt)\n\u001b[1;32m    612\u001b[0m torch\u001b[38;5;241m.\u001b[39m_foreach_add_(exp_avg_sq_sqrt, eps)\n\u001b[0;32m--> 613\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_foreach_addcdiv_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_exp_avgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexp_avg_sq_sqrt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep_size\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["print(f\"Training for {num_epochs} epochs started.\")\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","\n","    for batch_idx, (data1, data2, data3, targets, _) in enumerate(train_loader):\n","\n","        data1 = data1.to(device)\n","        data2 = data2.to(device)\n","        data3 = data3.to(device)\n","        targets = targets.to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = model(data1, data2, data3)\n","\n","        pos_weight = targets*positive_weigh_factor  # All positive weights are equal to 10\n","        criterion = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n","        loss = criterion(outputs, targets)\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","        if batch_idx % 278 == 0:\n","            print(f\"Epoch {epoch+1}/{num_epochs}, Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item()}\")\n","\n","    scheduler.step()\n","    print(\"Scheduler:\",scheduler.state_dict())\n","    if epoch % 10 == 0:\n","        torch.save(model.state_dict(), f\"multimodal-model-epoch-{epoch}.pth\")\n","\n","# Save the trained model\n","model.eval()\n","torch.save(model.state_dict(), \"multimodal-model-final.pth\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Done\n"]}],"source":["model.eval()\n","print(\"Done\")"]},{"cell_type":"markdown","metadata":{},"source":["## Test Loop\n","\n","Again, nothing special, just a standard inference."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["full_preds = []\n","label_cols = None\n","\n","with torch.no_grad():\n","    all_predictions = []\n","    surveys = []\n","    top_k_indices = None\n","    full_preds = None\n","    for batch_idx, (data1, data2, data3, surveyID) in enumerate(test_loader):\n","\n","        data1 = data1.to(device)\n","        data2 = data2.to(device)\n","        data3 = data3.to(device)\n","        targets = targets.to(device)\n","\n","        outputs = model(data1, data2, data3)\n","\n","        predictions = torch.sigmoid(outputs).cpu()\n","        label_cols = predictions\n","        predictions = predictions.numpy()\n","        if full_preds is None:\n","            full_preds = predictions\n","        else:\n","            full_preds = np.concatenate((full_preds, predictions), axis=0)\n","\n","        # Select top-25 values as predictions\n","        top_25 = np.argsort(-predictions, axis=1)[:, :25]\n","        if top_k_indices is None:\n","            top_k_indices = top_25\n","        else:\n","            top_k_indices = np.concatenate((top_k_indices, top_25), axis=0)\n","\n","        surveys.extend(surveyID.cpu().numpy())"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"tags":[],"trusted":true},"outputs":[],"source":["# with torch.no_grad():\n","#     all_predictions = []\n","#     surveys = []\n","#     top_k_indices = None\n","#     for batch_idx, (data1, data2, data3, surveyID) in enumerate(test_loader):\n","\n","#         data1 = data1.to(device)\n","#         data2 = data2.to(device)\n","#         data3 = data3.to(device)\n","#         targets = targets.to(device)\n","\n","#         outputs = model(data1, data2, data3)\n","#         predictions = torch.sigmoid(outputs).cpu().numpy()\n","\n","#         # Sellect top-25 values as predictions\n","#         top_25 = np.argsort(-predictions, axis=1)[:, :20]\n","#         if top_k_indices is None:\n","#             top_k_indices = top_25\n","#         else:\n","#             top_k_indices = np.concatenate((top_k_indices, top_25), axis=0)\n","\n","#         surveys.extend(surveyID.cpu().numpy())"]},{"cell_type":"markdown","metadata":{},"source":["## Save prediction file! 🎉🥳🙌🤗"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"trusted":true},"outputs":[],"source":["data_concatenated = [' '.join(map(str, row)) for row in top_k_indices]\n","\n","pd.DataFrame(\n","    {'surveyId': surveys,\n","     'predictions': data_concatenated,\n","    }).to_csv(\"submission2.csv\", index = False)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["preddf = pd.DataFrame(full_preds).add_prefix(\"speciesId_\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["preddf.to_pickle(\"pred_improved_norm_df.pkl\")"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"databundleVersionId":8171035,"sourceId":64733,"sourceType":"competition"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":4}
