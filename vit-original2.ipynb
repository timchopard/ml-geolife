{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17159bbd",
   "metadata": {
    "papermill": {
     "duration": 0.006704,
     "end_time": "2024-05-05T14:42:28.165291",
     "exception": false,
     "start_time": "2024-05-05T14:42:28.158587",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# This is an INFERENCE using the model combine the method of MLP, CNN and Transformer\n",
    "\n",
    "Thanks PICEKL for his baseline, which help me a lot.\n",
    "\n",
    "The main idea of this method is to obtain multiple features from multiple dimensions. We simply divide it into four types: **image information** with spatial characteristics, **meta information** that is independent of each other, **climate information** and **satellite information** with time characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1703a0e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T21:25:07.29831Z",
     "start_time": "2024-04-30T21:25:05.354584Z"
    },
    "execution": {
     "iopub.execute_input": "2024-05-05T14:42:41.363659Z",
     "iopub.status.busy": "2024-05-05T14:42:41.363369Z",
     "iopub.status.idle": "2024-05-05T14:42:49.299669Z",
     "shell.execute_reply": "2024-05-05T14:42:49.298878Z"
    },
    "papermill": {
     "duration": 7.946835,
     "end_time": "2024-05-05T14:42:49.302146",
     "exception": false,
     "start_time": "2024-05-05T14:42:41.355311",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/startung/miniconda3/envs/pt/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/startung/miniconda3/envs/pt/lib/python3.11/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops.layers.torch import Rearrange\n",
    "from einops import rearrange, repeat\n",
    "from models2 import MLP, ViT, ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c4fae07",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(\"output\") / time.strftime('%Y-%m-%d_%H%M', time.localtime())\n",
    "path.mkdir(parents=True, exist_ok=True)\n",
    "logger = logging.getLogger()\n",
    "fhandler = logging.FileHandler(filename=path / 'log.txt', mode='a')\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(fhandler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbb163e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp = {\n",
    "    \"learning_rate\": 0.00025,\n",
    "    \"num_epochs\": 5,\n",
    "    \"positive_weight_factor\": 1.0,\n",
    "    \"INITIAL_SEED\": 113,\n",
    "    \"test_batch_size\": 1,\n",
    "    \"train_batch_size\": 64,\n",
    "    \"num_classes\": 11255, # max 11255\n",
    "    \"data_root\": \"data\", #\"/kaggle/input\"\n",
    "    \"train_val_split\": 0.8, # the fraction of the data used for training e.g. 0.8\n",
    "    \"vit_path\": None, # \"output/2024-05-21_1613\",\n",
    "    \"data_path\": \"output/2024-05-21_1613\",\n",
    "}\n",
    "vit_path = None if hp[\"vit_path\"] is None else Path(hp[\"vit_path\"])\n",
    "data_path = None if hp[\"data_path\"] is None else Path(hp[\"data_path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5ef433d",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f\"Hyperparameters: {hp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c275e9a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-05T14:42:49.317022Z",
     "iopub.status.busy": "2024-05-05T14:42:49.316581Z",
     "iopub.status.idle": "2024-05-05T14:42:49.320755Z",
     "shell.execute_reply": "2024-05-05T14:42:49.319956Z"
    },
    "papermill": {
     "duration": 0.013539,
     "end_time": "2024-05-05T14:42:49.322641",
     "exception": false,
     "start_time": "2024-05-05T14:42:49.309102",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_classes = 11255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f91a65",
   "metadata": {
    "papermill": {
     "duration": 0.006362,
     "end_time": "2024-05-05T14:42:49.335533",
     "exception": false,
     "start_time": "2024-05-05T14:42:49.329171",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "When reading data, different **fusion** and **normalization** will be performed for different types of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "915eb161",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, subset):\n",
    "        self.subset = subset\n",
    "        self.metadata_path = f\"{hp['data_root']}/geolifeclef-2024/GLC24_PA_metadata_{self.subset}.csv\"\n",
    "        self.metadata = pd.read_csv(self.metadata_path)\n",
    "        self.transform = transforms.Compose([transforms.Resize((128,128)), transforms.ToTensor()])\n",
    "        self.merge_key = 'surveyId'\n",
    "\n",
    "        self.metadata_data = self.Norm(self.process_metadata())\n",
    "        self.climate_data = self.Norm_all(self.process_climate())\n",
    "        self.landsat_data = self.process_landsat()\n",
    "        self.elevation_data = self.Norm(self.process_elevation())\n",
    "        self.human_footprint_data = self.Norm(self.process_human_footprint())\n",
    "        self.landcover_data = self.Norm(self.process_landcover())\n",
    "        self.soilgrids_data = self.Norm(self.process_soilgrids())\n",
    "        self.metadata_data = torch.cat((self.metadata_data, self.elevation_data, self.human_footprint_data, self.landcover_data, self.soilgrids_data), dim=1)\n",
    "\n",
    "    def process_metadata(self):\n",
    "        metadata = self.metadata.drop_duplicates(subset=\"surveyId\").reset_index(drop=True).copy()\n",
    "        metadata.fillna(0,inplace=True)\n",
    "        metadata.replace({float('-inf'): 0}, inplace=True)\n",
    "        return metadata.iloc[:,:5]\n",
    "\n",
    "    def process_climate(self):\n",
    "        climate_average = pd.read_csv(f\"{hp['data_root']}/geolifeclef-2024/EnvironmentalRasters/EnvironmentalRasters/Climate/Average 1981-2010/GLC24-PA-{self.subset}-bioclimatic.csv\")\n",
    "        climate_monthly = pd.read_csv(f\"{hp['data_root']}/geolifeclef-2024/EnvironmentalRasters/EnvironmentalRasters/Climate/Monthly/GLC24-PA-{self.subset}-bioclimatic_monthly.csv\")\n",
    "        climate = pd.merge(climate_average, climate_monthly, on=self.merge_key)\n",
    "        climate.fillna(climate.mean(),inplace=True)\n",
    "        return climate\n",
    "\n",
    "    def process_landsat(self):\n",
    "        landsat_types = ['blue', 'green', 'red', 'nir', 'swir1', 'swir2']\n",
    "        landsat_dfs = []\n",
    "        for landsat_type in landsat_types:\n",
    "            landsat = pd.read_csv(f\"{hp['data_root']}/geolifeclef-2024/PA-{self.subset}-landsat_time_series/GLC24-PA-{self.subset}-landsat_time_series-{landsat_type}.csv\")\n",
    "            landsat.fillna(landsat.mean(),inplace=True)\n",
    "            landsat_dfs.append(landsat)\n",
    "        return torch.cat([self.Norm_all(landsat_df) for landsat_df in landsat_dfs],axis=1)\n",
    "\n",
    "    def process_elevation(self):\n",
    "        elevation = pd.read_csv(f\"{hp['data_root']}/geolifeclef-2024/EnvironmentalRasters/EnvironmentalRasters/Elevation/GLC24-PA-{self.subset}-elevation.csv\")\n",
    "        elevation[elevation<0]=0\n",
    "        elevation.fillna(elevation.mean(),inplace=True)\n",
    "        return elevation\n",
    "\n",
    "    def process_human_footprint(self):\n",
    "        human_footprint = pd.read_csv(f\"{hp['data_root']}/geolifeclef-2024/EnvironmentalRasters/EnvironmentalRasters/Human Footprint/GLC24-PA-{self.subset}-human_footprint.csv\")\n",
    "        human_footprint[human_footprint<0]=0\n",
    "        human_footprint.fillna(human_footprint.mean(),inplace=True)\n",
    "        return human_footprint\n",
    "\n",
    "    def process_landcover(self):\n",
    "        landcover = pd.read_csv(f\"{hp['data_root']}/geolifeclef-2024/EnvironmentalRasters/EnvironmentalRasters/LandCover/GLC24-PA-{self.subset}-landcover.csv\")\n",
    "        landcover[landcover<0]=0\n",
    "        landcover.fillna(landcover.mean(),inplace=True)\n",
    "        return landcover\n",
    "\n",
    "    def process_soilgrids(self):\n",
    "        soilgrids = pd.read_csv(f\"{hp['data_root']}/geolifeclef-2024/EnvironmentalRasters/EnvironmentalRasters/SoilGrids/GLC24-PA-{self.subset}-soilgrids.csv\")\n",
    "        soilgrids[soilgrids<0]=0\n",
    "        soilgrids.fillna(soilgrids.mean(),inplace=True)\n",
    "        return soilgrids\n",
    "\n",
    "    def Norm(self,df):\n",
    "        output=torch.from_numpy(df.iloc[:,1:].values).float()\n",
    "        return (output-output.mean(dim=0))/output.std(dim=0)\n",
    "\n",
    "    def Norm_all(self,df):\n",
    "        output=torch.from_numpy(df.iloc[:,1:].values).float()\n",
    "        return (output-output.mean())/output.std()\n",
    "\n",
    "    def patch_rgb_path(self,survey_id):\n",
    "        path = f\"{hp['data_root']}/geolifeclef-2024/PA_{self.subset.title()}_SatellitePatches_RGB/pa_{self.subset}_patches_rgb\"\n",
    "        for d in (str(survey_id)[-2:], str(survey_id)[-4:-2]):\n",
    "            path = os.path.join(path, d)\n",
    "        path = os.path.join(path, f\"{survey_id}.jpeg\")\n",
    "        return path\n",
    "\n",
    "    def patch_nir_path(self,survey_id):\n",
    "        path = f\"{hp['data_root']}/geolifeclef-2024/PA_{self.subset.title()}_SatellitePatches_NIR/pa_{self.subset}_patches_nir\"\n",
    "        for d in (str(survey_id)[-2:], str(survey_id)[-4:-2]):\n",
    "            path = os.path.join(path, d)\n",
    "        path = os.path.join(path, f\"{survey_id}.jpeg\")\n",
    "        return path\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        survey_id = self.metadata.surveyId[idx]\n",
    "        image_path = self.patch_rgb_path(survey_id)\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        image = self.transform(image)\n",
    "        image = image.unsqueeze(0)\n",
    "        image_nir_path = self.patch_nir_path(survey_id)\n",
    "        nir_image = Image.open(image_nir_path).convert(\"L\")\n",
    "        nir_image = self.transform(nir_image)\n",
    "        nir_image = nir_image.unsqueeze(0)\n",
    "        image_data = torch.cat([image,nir_image],dim=1)\n",
    "        image_data = torch.squeeze(image_data)\n",
    "        sample=[self.metadata_data[idx,:],image_data,self.landsat_data[idx,:],self.climate_data[idx,:]]\n",
    "        return sample, survey_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cabf889",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(CustomDataset):\n",
    "    def __init__(self):\n",
    "        super().__init__(subset=\"test\")\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample, survey_id = super().__getitem__(idx)\n",
    "        return sample, survey_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "079ed4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(CustomDataset):\n",
    "    def __init__(self):\n",
    "        super().__init__(subset=\"train\")\n",
    "        labels = self.metadata[['surveyId' ,'speciesId']].astype(int).copy()\n",
    "        self.label_dict = labels.groupby('surveyId')['speciesId'].apply(list).to_dict()\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample, survey_id = super().__getitem__(idx)\n",
    "        species_ids = self.label_dict[survey_id]  # Get list of species IDs for the survey ID\n",
    "        label = torch.zeros(hp['num_classes'])  # Initialize label tensor\n",
    "        for species_id in species_ids:\n",
    "            label_id = species_id\n",
    "            label[label_id] = 1  # Set the corresponding class index to 1 for each species ID\n",
    "        count = len(species_ids)\n",
    "        return sample, survey_id, label, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39b64182",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T21:25:34.532017Z",
     "start_time": "2024-04-30T21:25:32.615562Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-05-05T14:42:49.391704Z",
     "iopub.status.busy": "2024-05-05T14:42:49.391421Z",
     "iopub.status.idle": "2024-05-05T14:42:51.305244Z",
     "shell.execute_reply": "2024-05-05T14:42:51.304427Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 1.923732,
     "end_time": "2024-05-05T14:42:51.307566",
     "exception": false,
     "start_time": "2024-05-05T14:42:49.383834",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dataset and DataLoader\n",
    "batch_size = 1\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize((128,128)),\n",
    "#     transforms.ToTensor()\n",
    "# ])\n",
    "\n",
    "# Load Training metadata\n",
    "test_dataset = TestDataset()\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=1)\n",
    "\n",
    "train_dataset = TrainDataset()\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad70c7af",
   "metadata": {
    "papermill": {
     "duration": 0.006496,
     "end_time": "2024-05-05T14:42:51.321299",
     "exception": false,
     "start_time": "2024-05-05T14:42:51.314803",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This is an MLP used to extract features from generally independent information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7143060",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-05T14:42:51.335647Z",
     "iopub.status.busy": "2024-05-05T14:42:51.335345Z",
     "iopub.status.idle": "2024-05-05T14:42:51.341096Z",
     "shell.execute_reply": "2024-05-05T14:42:51.340279Z"
    },
    "papermill": {
     "duration": 0.015091,
     "end_time": "2024-05-05T14:42:51.342937",
     "exception": false,
     "start_time": "2024-05-05T14:42:51.327846",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(dim, out_dim*5)\n",
    "        self.fc2 = nn.Linear(out_dim*5, out_dim)\n",
    "        self.norm = nn.LayerNorm(out_dim*5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.tanh(self.fc1(x))\n",
    "        x = self.norm(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b334f1d",
   "metadata": {
    "papermill": {
     "duration": 0.006256,
     "end_time": "2024-05-05T14:42:51.490923",
     "exception": false,
     "start_time": "2024-05-05T14:42:51.484667",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This is the **final multi-modal model**. Unlike ViT, its input is features extracted from each dimension. \n",
    "\n",
    "For features with time information, I added additional position information. \n",
    "\n",
    "At the same time, I also added a feature that is a fusion of the first four features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f8dde92",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-05T14:42:51.505245Z",
     "iopub.status.busy": "2024-05-05T14:42:51.504543Z",
     "iopub.status.idle": "2024-05-05T14:42:51.515887Z",
     "shell.execute_reply": "2024-05-05T14:42:51.515235Z"
    },
    "papermill": {
     "duration": 0.020283,
     "end_time": "2024-05-05T14:42:51.517661",
     "exception": false,
     "start_time": "2024-05-05T14:42:51.497378",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MutiModal(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(MutiModal, self).__init__()\n",
    "        self.cls = nn.Parameter(torch.randn(1, 1, 200))\n",
    "        self.meta = Embedding(31,200)\n",
    "        self.resnet18 = ResNet18(200)\n",
    "        self.landsat = Embedding(504,200)\n",
    "        self.position_landsat = nn.Parameter(torch.randn(1, 504))\n",
    "        self.climate = Embedding(931,200)\n",
    "        self.position_climate = nn.Parameter(torch.randn(1, 931))\n",
    "        self.emb = Embedding(800,200)\n",
    "        self.position_combine = nn.Parameter(torch.randn(1, 800))\n",
    "        self.vit = ViT(200, 2, 200, 400, num_classes)\n",
    "        self.position = nn.Parameter(torch.randn(1, 6, 200))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch = x[0].size(0)\n",
    "        CLS = repeat(self.cls, '1 1 d -> b 1 d', b=batch).to(device)\n",
    "        META = self.meta(x[0])\n",
    "        IMG = self.resnet18(x[1])\n",
    "        LANDSAT = self.landsat(x[2]+self.position_landsat)\n",
    "        CLIMATE = self.climate(x[3]+self.position_climate)\n",
    "        combine = torch.cat((META, IMG, LANDSAT, CLIMATE), dim=1)\n",
    "        COMBINE = self.emb(combine+self.position_combine)\n",
    "        token = torch.concat((CLS, META.unsqueeze(1)), dim=1)\n",
    "        token = torch.concat((token, IMG.unsqueeze(1)), dim=1)\n",
    "        token = torch.concat((token, LANDSAT.unsqueeze(1)), dim=1)\n",
    "        token = torch.concat((token, CLIMATE.unsqueeze(1)), dim=1)\n",
    "        token = torch.concat((token, COMBINE.unsqueeze(1)), dim=1)\n",
    "        out = self.vit(token+self.position)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8e1f40e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T21:25:31.611823Z",
     "start_time": "2024-04-30T21:25:31.607373Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-05-05T14:42:51.531532Z",
     "iopub.status.busy": "2024-05-05T14:42:51.531262Z",
     "iopub.status.idle": "2024-05-05T14:42:52.798635Z",
     "shell.execute_reply": "2024-05-05T14:42:52.797682Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 1.276491,
     "end_time": "2024-05-05T14:42:52.800566",
     "exception": false,
     "start_time": "2024-05-05T14:42:51.524075",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE = CUDA\n"
     ]
    }
   ],
   "source": [
    "# Check if cuda is available\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"DEVICE = CUDA\")\n",
    "\n",
    "model = MutiModal(num_classes).to(device)\n",
    "# model.load_state_dict(torch.load(\"models/vit/Model.pth\", map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da887bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/startung/miniconda3/envs/pt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=hp['learning_rate'])\n",
    "logger.info(\"Optimizer: AdamW\")\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=25, verbose=True)\n",
    "logger.info(\"Scheduler: CosineAnnealingLR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a14d814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 5 epochs started.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Batch 0/1391, Loss: 0.7319745421409607\n",
      "Epoch 1/5, Batch 139/1391, Loss: 0.03292905539274216\n",
      "Epoch 1/5, Batch 278/1391, Loss: 0.016077920794487\n",
      "Epoch 1/5, Batch 417/1391, Loss: 0.011257714591920376\n",
      "Epoch 1/5, Batch 556/1391, Loss: 0.011127620935440063\n",
      "Epoch 1/5, Batch 695/1391, Loss: 0.00969545729458332\n",
      "Epoch 1/5, Batch 834/1391, Loss: 0.009522094391286373\n",
      "Epoch 1/5, Batch 973/1391, Loss: 0.008371397852897644\n",
      "Epoch 1/5, Batch 1112/1391, Loss: 0.009551092982292175\n",
      "Epoch 1/5, Batch 1251/1391, Loss: 0.009219029918313026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [13:00<52:02, 780.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Batch 1390/1391, Loss: 0.009186108596622944\n",
      "Epoch 2/5, Batch 0/1391, Loss: 0.008382325060665607\n",
      "Epoch 2/5, Batch 139/1391, Loss: 0.008062752895057201\n",
      "Epoch 2/5, Batch 278/1391, Loss: 0.007518880534917116\n",
      "Epoch 2/5, Batch 417/1391, Loss: 0.007663054391741753\n",
      "Epoch 2/5, Batch 556/1391, Loss: 0.009462274610996246\n",
      "Epoch 2/5, Batch 695/1391, Loss: 0.007674806285649538\n",
      "Epoch 2/5, Batch 834/1391, Loss: 0.008438208140432835\n",
      "Epoch 2/5, Batch 973/1391, Loss: 0.007647836115211248\n",
      "Epoch 2/5, Batch 1112/1391, Loss: 0.007479742169380188\n",
      "Epoch 2/5, Batch 1251/1391, Loss: 0.0071536884643137455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [26:02<39:03, 781.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Batch 1390/1391, Loss: 0.006751437671482563\n",
      "Epoch 3/5, Batch 0/1391, Loss: 0.007217148318886757\n",
      "Epoch 3/5, Batch 139/1391, Loss: 0.006697237957268953\n",
      "Epoch 3/5, Batch 278/1391, Loss: 0.0071390969678759575\n",
      "Epoch 3/5, Batch 417/1391, Loss: 0.006971823051571846\n",
      "Epoch 3/5, Batch 556/1391, Loss: 0.006098734214901924\n",
      "Epoch 3/5, Batch 695/1391, Loss: 0.006469473708420992\n",
      "Epoch 3/5, Batch 834/1391, Loss: 0.006787928286939859\n",
      "Epoch 3/5, Batch 973/1391, Loss: 0.006482400000095367\n",
      "Epoch 3/5, Batch 1112/1391, Loss: 0.005802351515740156\n",
      "Epoch 3/5, Batch 1251/1391, Loss: 0.005611401051282883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [39:05<26:03, 781.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, Batch 1390/1391, Loss: 0.005501255393028259\n",
      "Epoch 4/5, Batch 0/1391, Loss: 0.006081508006900549\n",
      "Epoch 4/5, Batch 139/1391, Loss: 0.005711866542696953\n",
      "Epoch 4/5, Batch 278/1391, Loss: 0.005280612502247095\n",
      "Epoch 4/5, Batch 417/1391, Loss: 0.0051314434967935085\n",
      "Epoch 4/5, Batch 556/1391, Loss: 0.00486820749938488\n",
      "Epoch 4/5, Batch 695/1391, Loss: 0.004583961330354214\n",
      "Epoch 4/5, Batch 834/1391, Loss: 0.0049757882952690125\n",
      "Epoch 4/5, Batch 973/1391, Loss: 0.0051056803204119205\n",
      "Epoch 4/5, Batch 1112/1391, Loss: 0.004967124667018652\n",
      "Epoch 4/5, Batch 1251/1391, Loss: 0.004101336933672428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [52:08<13:02, 782.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, Batch 1390/1391, Loss: 0.00511839147657156\n",
      "Epoch 5/5, Batch 0/1391, Loss: 0.004988267086446285\n",
      "Epoch 5/5, Batch 139/1391, Loss: 0.004751879256218672\n",
      "Epoch 5/5, Batch 278/1391, Loss: 0.0050101024098694324\n",
      "Epoch 5/5, Batch 417/1391, Loss: 0.004253692924976349\n",
      "Epoch 5/5, Batch 556/1391, Loss: 0.004044508561491966\n",
      "Epoch 5/5, Batch 695/1391, Loss: 0.004495963454246521\n",
      "Epoch 5/5, Batch 834/1391, Loss: 0.004505382850766182\n",
      "Epoch 5/5, Batch 973/1391, Loss: 0.004305401351302862\n",
      "Epoch 5/5, Batch 1112/1391, Loss: 0.003823834005743265\n",
      "Epoch 5/5, Batch 1251/1391, Loss: 0.0038854144513607025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [1:05:12<00:00, 782.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Batch 1390/1391, Loss: 0.004272107966244221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if hp['vit_path'] is not None:\n",
    "    model.load_state_dict(torch.load(vit_path / \"multimodal.pth\"))\n",
    "    logger.info(f\"Model loaded: {vit_path / 'multimodal.pth'}\")\n",
    "else:\n",
    "    print(f\"Training for {hp['num_epochs']} epochs started.\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in tqdm(range(hp['num_epochs'])):\n",
    "        model.train()\n",
    "\n",
    "        for batch_idx, (sample, survey_id, labels, count) in enumerate(train_loader):\n",
    "            samples = [tensor.to(device) for tensor in sample]\n",
    "            survey_id = survey_id.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.set_grad_enabled(True):\n",
    "                outputs = model(samples)\n",
    "\n",
    "                pos_weight = labels*hp['positive_weight_factor']  # All positive weights are equal to 10\n",
    "                criterion = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            if batch_idx % (len(train_loader)//10) == 0:\n",
    "                print(f\"Epoch {epoch+1}/{hp['num_epochs']}, Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item()}\")\n",
    "\n",
    "        scheduler.step()\n",
    "        logger.info(\"Scheduler:\",scheduler.state_dict())\n",
    "\n",
    "    logger.info(f\"Training time: {(time.time()-start_time)/60:.0f} minutes\")\n",
    "    # Save the trained model\n",
    "    logger.info(model.eval())\n",
    "    torch.save(model.state_dict(), path / \"multimodal.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ad6677f",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-30T21:25:34.536634Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-05-05T14:42:52.815910Z",
     "iopub.status.busy": "2024-05-05T14:42:52.815177Z",
     "iopub.status.idle": "2024-05-05T14:44:06.291126Z",
     "shell.execute_reply": "2024-05-05T14:44:06.288992Z"
    },
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 73.485486,
     "end_time": "2024-05-05T14:44:06.293057",
     "exception": false,
     "start_time": "2024-05-05T14:42:52.807571",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4716/4716 [00:26<00:00, 180.80it/s]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    surveys = []\n",
    "    top_indices = []\n",
    "    for data, surveyID in tqdm(test_loader, total=len(test_loader)):\n",
    "\n",
    "        data = [tensor.to(device) for tensor in data]\n",
    "\n",
    "        outputs = model(data)\n",
    "        predictions = torch.sigmoid(outputs).cpu().numpy()\n",
    "        predictions = np.squeeze(predictions)\n",
    "        prediction = np.argwhere(predictions>=0.95).flatten()\n",
    "        top_indices.append(prediction)\n",
    "        surveys.extend(surveyID.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6632f95d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-05T14:44:06.404548Z",
     "iopub.status.busy": "2024-05-05T14:44:06.404201Z",
     "iopub.status.idle": "2024-05-05T14:44:06.506275Z",
     "shell.execute_reply": "2024-05-05T14:44:06.505581Z"
    },
    "papermill": {
     "duration": 0.159619,
     "end_time": "2024-05-05T14:44:06.508072",
     "exception": false,
     "start_time": "2024-05-05T14:44:06.348453",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_concatenated = [' '.join(map(str, row)) for row in top_indices]\n",
    "\n",
    "pd.DataFrame(\n",
    "    {'surveyId': surveys,\n",
    "     'predictions': data_concatenated,\n",
    "    }).to_csv(f\"submission{time.strftime('%Y-%m-%d_%H%M', time.localtime())}.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 8171035,
     "sourceId": 64733,
     "sourceType": "competition"
    },
    {
     "isSourceIdPinned": true,
     "modelInstanceId": 36765,
     "sourceId": 43782,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 102.728954,
   "end_time": "2024-05-05T14:44:08.185017",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-05-05T14:42:25.456063",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
