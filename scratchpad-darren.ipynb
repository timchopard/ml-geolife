{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data\n",
      "\n",
      "Load data\n",
      "Drop countries not in test data\n",
      "Replace -inf, inf with NaN\n",
      "Process NaN values\n",
      " - Processing train data: areaInM2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:01<00:00, 14.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Processing train data: geoUncertaintyInM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:01<00:00, 14.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Processing test data: areaInM2\n",
      " - Processing test data: geoUncertaintyInM\n",
      "Resulting dataframes\n",
      " - Train data: (1460051, 9)\n",
      " - Test data: (4716, 8)\n",
      "Set speciesId as int\n",
      "Resulting dataframes\n",
      " - Train data: (1460051, 9)\n",
      " - Test data: (4716, 8)\n",
      "Combine environmental data\n",
      " - Processing train data: data/EnvironmentalRasters/EnvironmentalRasters/Climate/Average 1981-2010/GLC24-PA-train-bioclimatic.csv\n",
      " - Processing train data: data/EnvironmentalRasters/EnvironmentalRasters/Climate/Monthly/GLC24-PA-train-bioclimatic_monthly.csv\n",
      " - Processing train data: data/EnvironmentalRasters/EnvironmentalRasters/Elevation/GLC24-PA-train-elevation.csv\n",
      " - Processing train data: data/EnvironmentalRasters/EnvironmentalRasters/Human Footprint/GLC24-PA-train-human_footprint.csv\n",
      " - Processing train data: data/EnvironmentalRasters/EnvironmentalRasters/LandCover/GLC24-PA-train-landcover.csv\n",
      " - Processing train data: data/EnvironmentalRasters/EnvironmentalRasters/SoilGrids/GLC24-PA-train-soilgrids.csv\n",
      " - Processing test data: data/EnvironmentalRasters/EnvironmentalRasters/Climate/Average 1981-2010/GLC24-PA-test-bioclimatic.csv\n",
      " - Processing test data: data/EnvironmentalRasters/EnvironmentalRasters/Climate/Monthly/GLC24-PA-test-bioclimatic_monthly.csv\n",
      " - Processing test data: data/EnvironmentalRasters/EnvironmentalRasters/Elevation/GLC24-PA-test-elevation.csv\n",
      " - Processing test data: data/EnvironmentalRasters/EnvironmentalRasters/Human Footprint/GLC24-PA-test-human_footprint.csv\n",
      " - Processing test data: data/EnvironmentalRasters/EnvironmentalRasters/LandCover/GLC24-PA-test-landcover.csv\n",
      " - Processing test data: data/EnvironmentalRasters/EnvironmentalRasters/SoilGrids/GLC24-PA-test-soilgrids.csv\n",
      "Resulting dataframes\n",
      " - Train data: (1460051, 967)\n",
      " - Test data: (4716, 966)\n",
      "Handle missing data\n",
      " - Processing train data: Elevation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:09<00:00,  1.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Processing train data: HumanFootprint-NavWater1994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:09<00:00,  1.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Processing train data: HumanFootprint-NavWater2009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:09<00:00,  1.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Processing train data: HumanFootprint-Roads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:09<00:00,  1.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Processing train data: HumanFootprint-HFP1993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:09<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Processing train data: HumanFootprint-HFP2009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:09<00:00,  1.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Processing train data: Soilgrid-bdod\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:09<00:00,  1.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Processing train data: Soilgrid-cec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:09<00:00,  1.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Processing train data: Soilgrid-cfvo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:09<00:00,  1.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Processing train data: Soilgrid-clay\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:09<00:00,  1.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Processing train data: Soilgrid-nitrogen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:09<00:00,  1.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Processing train data: Soilgrid-phh2o\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:09<00:00,  1.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Processing train data: Soilgrid-sand\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:09<00:00,  1.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Processing train data: Soilgrid-silt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:09<00:00,  1.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Processing train data: Soilgrid-soc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:09<00:00,  1.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Processing train data: Soilgrid-bdod\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:05<00:00,  3.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Processing train data: Soilgrid-soc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:05<00:00,  3.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Processing train data: Soilgrid-cec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:05<00:00,  3.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Processing train data: Soilgrid-cfvo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:05<00:00,  3.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Processing train data: Soilgrid-clay\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:05<00:00,  3.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Processing train data: Soilgrid-nitrogen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:05<00:00,  3.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Processing train data: Soilgrid-phh2o\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:05<00:00,  3.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Processing train data: Soilgrid-sand\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:05<00:00,  3.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Processing train data: Soilgrid-silt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:05<00:00,  3.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Processing train data: HumanFootprint-NavWater1994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:05<00:00,  3.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Processing train data: HumanFootprint-NavWater2009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:05<00:00,  3.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Processing train data: HumanFootprint-Roads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:05<00:00,  3.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Processing train data: HumanFootprint-HFP1993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:05<00:00,  3.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Processing train data: HumanFootprint-HFP2009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:05<00:00,  3.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resulting dataframes\n",
      " - Train data: (1460051, 967)\n",
      " - Test data: (4716, 966)\n",
      "combined_train_df.index=RangeIndex(start=0, stop=1460051, step=1)\n",
      "combined_test_df.index=RangeIndex(start=0, stop=4716, step=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Data Processor\n",
    "#\n",
    "# Desired output\n",
    "# - Full train dataset - for training model for Kaggle submission\n",
    "# - Full test dataset - for Kaggle submission\n",
    "# - Internal train dataset - 80% of full train dataset for internal banchmarking\n",
    "# - Internal test dataset - 20% of full train dataset for internal benchmarking\n",
    "# - All tabular data in one file\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "print(\"Processing data\\n\")\n",
    "\n",
    "# Load data\n",
    "print(\"Load data\")\n",
    "original_train_df = pd.read_csv(\"data/GLC24_PA_metadata_train.csv\")\n",
    "original_test_df = pd.read_csv(\"data/GLC24_PA_metadata_test.csv\")\n",
    "metadata_train_df = original_train_df.copy()\n",
    "metadata_test_df = original_test_df.copy()\n",
    "\n",
    "# Drop: 'Andorra', 'Hungary', 'Ireland', 'Latvia', 'Luxembourg', 'Monaco', 'Norway',\n",
    "# 'Portugal', 'Romania', 'Serbia', 'The former Yugoslav Republic of Macedonia' they\n",
    "# are not represented in the test set\n",
    "print(\"Drop countries not in test data\")\n",
    "drop_countries = {\n",
    "    \"Andorra\",\n",
    "    \"Hungary\",\n",
    "    \"Ireland\",\n",
    "    \"Latvia\",\n",
    "    \"Luxembourg\",\n",
    "    \"Monaco\",\n",
    "    \"Norway\",\n",
    "    \"Portugal\",\n",
    "    \"Romania\",\n",
    "    \"Serbia\",\n",
    "    \"The former Yugoslav Republic of Macedonia\",\n",
    "}\n",
    "metadata_train_df = metadata_train_df[~metadata_train_df.country.isin(drop_countries)]\n",
    "\n",
    "# -inf, inf replaced by NaN\n",
    "print(\"Replace -inf, inf with NaN\")\n",
    "metadata_train_df = metadata_train_df.replace([np.inf, -np.inf], np.nan)\n",
    "metadata_test_df = metadata_test_df.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# For the train data NaN in geoUncertaintyInM, and areaInM2 replaced with the country median values\n",
    "print(\"Process NaN values\")\n",
    "for column in [\"areaInM2\", \"geoUncertaintyInM\"]:\n",
    "    print(f\" - Processing train data: {column}\")\n",
    "    for country in tqdm(metadata_train_df.country.unique()):\n",
    "        metadata_train_df.loc[\n",
    "            metadata_train_df.country == country, column\n",
    "        ] = metadata_train_df.query(f\"country == '{country}'\")[column].fillna(\n",
    "            metadata_train_df.query(f\"country == '{country}'\")[column].median()\n",
    "        )\n",
    "\n",
    "# For the test data NaN in geoUncertaintyInM, and areaInM2 replaced with the training data country\n",
    "# median values (note this is data leakage, but I believe it is tolerable)\n",
    "for column in [\"areaInM2\", \"geoUncertaintyInM\"]:\n",
    "    print(f\" - Processing test data: {column}\")\n",
    "    for country in metadata_test_df.country.unique():\n",
    "        metadata_test_df.loc[\n",
    "            metadata_test_df.country == country, column\n",
    "        ] = metadata_test_df.query(f\"country == '{country}'\")[column].fillna(\n",
    "            metadata_train_df.query(f\"country == '{country}'\")[column].median()\n",
    "        )\n",
    "\n",
    "# Resulting dataframes\n",
    "print(\"Resulting dataframes\")\n",
    "print(f\" - Train data: {metadata_train_df.shape}\")\n",
    "print(f\" - Test data: {metadata_test_df.shape}\")\n",
    "\n",
    "# set speciesId as int\n",
    "print(\"Set speciesId as int\")\n",
    "metadata_train_df[\"speciesId\"] = metadata_train_df[\"speciesId\"].astype(int)\n",
    "\n",
    "# Resulting dataframes\n",
    "print(\"Resulting dataframes\")\n",
    "print(f\" - Train data: {metadata_train_df.shape}\")\n",
    "print(f\" - Test data: {metadata_test_df.shape}\")\n",
    "\n",
    "# Combine all environmental data\n",
    "print(\"Combine environmental data\")\n",
    "files_to_combine = [\n",
    "    \"data/EnvironmentalRasters/EnvironmentalRasters/Climate/Average 1981-2010/GLC24-PA-{}-bioclimatic.csv\",\n",
    "    \"data/EnvironmentalRasters/EnvironmentalRasters/Climate/Monthly/GLC24-PA-{}-bioclimatic_monthly.csv\",\n",
    "    \"data/EnvironmentalRasters/EnvironmentalRasters/Elevation/GLC24-PA-{}-elevation.csv\",\n",
    "    \"data/EnvironmentalRasters/EnvironmentalRasters/Human Footprint/GLC24-PA-{}-human_footprint.csv\",\n",
    "    \"data/EnvironmentalRasters/EnvironmentalRasters/LandCover/GLC24-PA-{}-landcover.csv\",\n",
    "    \"data/EnvironmentalRasters/EnvironmentalRasters/SoilGrids/GLC24-PA-{}-soilgrids.csv\",\n",
    "]\n",
    "combined_train_df = metadata_train_df\n",
    "combined_test_df = metadata_test_df\n",
    "\n",
    "for file in files_to_combine:\n",
    "    print(\" - Processing train data:\", file.format(\"train\"))\n",
    "    combined_train_df = pd.merge(\n",
    "        combined_train_df, pd.read_csv(file.format(\"train\")), on=\"surveyId\"\n",
    "    )\n",
    "\n",
    "for file in files_to_combine:\n",
    "    print(\" - Processing test data:\", file.format(\"test\"))\n",
    "    combined_test_df = pd.merge(\n",
    "        combined_test_df, pd.read_csv(file.format(\"test\")), on=\"surveyId\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Resulting dataframes\n",
    "print(\"Resulting dataframes\")\n",
    "print(f\" - Train data: {combined_train_df.shape}\")\n",
    "print(f\" - Test data: {combined_test_df.shape}\")\n",
    "\n",
    "# Handle missing data\n",
    "print(\"Handle missing data\")\n",
    "for column in list(\n",
    "    combined_train_df.isna()\n",
    "    .sum()[combined_train_df.isna().sum() > 0]\n",
    "    .keys()\n",
    "):\n",
    "    print(f\" - Processing train data: {column}\")\n",
    "    for country in tqdm(combined_train_df.country.unique()):\n",
    "        combined_train_df.loc[\n",
    "            combined_train_df.country == country, column\n",
    "        ] = combined_train_df.query(f\"country == '{country}'\")[column].fillna(\n",
    "            combined_train_df.query(f\"country == '{country}'\")[column].median()\n",
    "        )\n",
    "\n",
    "for column in list(\n",
    "    combined_train_df.isna()\n",
    "    .sum()[combined_train_df.isna().sum() > 0]\n",
    "    .keys()\n",
    "):\n",
    "    print(f\" - Processing train data: {column}\")\n",
    "    for country in tqdm(combined_train_df.country.unique()):\n",
    "        combined_train_df.loc[\n",
    "            combined_train_df.country == country, column\n",
    "        ] = combined_train_df.query(f\"country == '{country}'\")[column].fillna(\n",
    "            combined_train_df.query(f\"country == '{country}'\")[column].median()\n",
    "        )\n",
    "\n",
    "for column in list(\n",
    "    combined_test_df.isna()\n",
    "    .sum()[combined_test_df.isna().sum() > 0]\n",
    "    .sort_values(ascending=False)\n",
    "    .keys()\n",
    "):\n",
    "    print(f\" - Processing train data: {column}\")\n",
    "    for country in tqdm(combined_test_df.country.unique()):\n",
    "        combined_test_df.loc[\n",
    "            combined_test_df.country == country, column\n",
    "        ] = combined_test_df.query(f\"country == '{country}'\")[column].fillna(\n",
    "            combined_train_df.query(f\"country == '{country}'\")[column].median()\n",
    "        )\n",
    "# Resulting dataframes\n",
    "print(\"Resulting dataframes\")\n",
    "print(f\" - Train data: {combined_train_df.shape}\")\n",
    "print(f\" - Test data: {combined_test_df.shape}\")\n",
    "\n",
    "print(f\"{combined_train_df.index=}\")\n",
    "print(f\"{combined_test_df.index=}\")\n",
    "\n",
    "# for training data country, region, and speciesId one-hot encoded\n",
    "print(\"One-hot encode country, region, and speciesId in training data\")\n",
    "for column in [\"country\", \"region\", \"speciesId\"]:\n",
    "    print(f\" - Processing: {column}\")\n",
    "    ohe_train_df = pd.concat(\n",
    "        [combined_train_df, pd.get_dummies(combined_train_df[column], prefix=column)],\n",
    "        axis=1,\n",
    "    )\n",
    "    ohe_train_df = ohe_train_df.drop(columns=[column])\n",
    "\n",
    "# for test data country and region one-hot encoded\n",
    "print(\"One-hot encode country and region in test data\")\n",
    "for column in [\"country\", \"region\"]:\n",
    "    print(f\" - Processing: {column}\")\n",
    "    ohe_test_df = pd.concat(\n",
    "        [combined_test_df, pd.get_dummies(combined_test_df[column], prefix=column)],\n",
    "        axis=1,\n",
    "    )\n",
    "    ohe_test_df = ohe_test_df.drop(columns=[column])\n",
    "\n",
    "\n",
    "# Grouped by surveyId, use max\n",
    "print(\"Group by surveyId\")\n",
    "grouped_train_df = ohe_train_df.groupby(\"surveyId\", as_index=False).max()\n",
    "\n",
    "# Resulting dataframes\n",
    "print(\"Resulting dataframes\")\n",
    "print(f\" - Train data: {grouped_train_df.shape}\")\n",
    "print(f\" - Test data: {ohe_test_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original_train_df.shape=(1483637, 9)\n",
      "original_test_df.shape=(4716, 8)\n",
      "metadata_train_df.shape=(1460051, 9)\n",
      "metadata_test_df.shape=(4716, 8)\n",
      "combined_train_df.shape=(1460051, 967)\n",
      "combined_test_df.shape=(4716, 966)\n",
      "ohe_train_df.shape=(1460051, 5893)\n",
      "ohe_test_df.shape=(4716, 969)\n",
      "grouped_train_df.shape=(88428, 5892)\n"
     ]
    }
   ],
   "source": [
    "print(f\"{original_train_df.shape=}\")\n",
    "print(f\"{original_test_df.shape=}\")\n",
    "print(f\"{metadata_train_df.shape=}\")\n",
    "print(f\"{metadata_test_df.shape=}\")\n",
    "print(f\"{combined_train_df.shape=}\")\n",
    "print(f\"{combined_test_df.shape=}\")\n",
    "print(f\"{ohe_train_df.shape=}\")\n",
    "print(f\"{ohe_test_df.shape=}\")\n",
    "print(f\"{grouped_train_df.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Find NaN in grouped_train_df\n"
     ]
    }
   ],
   "source": [
    "# find nan in grouped_train_df\n",
    "print(\"Find NaN in grouped_train_df\")\n",
    "for column in ohe_test_df.columns:\n",
    "    if ohe_test_df[column].isna().sum() > 0:\n",
    "        print(f\" - {column}: {ohe_test_df[column].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([    212,     222,     243,     324,     333,     391,     410,     489,\n",
       "           590,     607,\n",
       "       ...\n",
       "       3919341, 3919365, 3919375, 3919517, 3919518, 3919553, 3919592, 3919620,\n",
       "       3919640, 3919655],\n",
       "      dtype='int64', name='surveyId', length=88428)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_train_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "speciesId\n",
       "540      21478\n",
       "4397     19441\n",
       "254      18113\n",
       "4499     15065\n",
       "10317    14538\n",
       "         ...  \n",
       "5806         1\n",
       "8272         1\n",
       "416          1\n",
       "3907         1\n",
       "8119         1\n",
       "Name: count, Length: 4927, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_train_df.speciesId.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RangeIndex(start=0, stop=4716, step=1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohe_test_df.index"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
