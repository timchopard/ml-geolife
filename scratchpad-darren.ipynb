{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'surveyId'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m original_train_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/GLC24_PA_metadata_train.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[43moriginal_train_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msurveyId\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msurveyId\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcount()\u001b[38;5;241m.\u001b[39msort_values(ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.11/site-packages/pandas/core/series.py:2238\u001b[0m, in \u001b[0;36mSeries.groupby\u001b[0;34m(self, by, axis, level, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[1;32m   2235\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas_index=False only valid with DataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2236\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_axis_number(axis)\n\u001b[0;32m-> 2238\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesGroupBy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2239\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2240\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2241\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2242\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2243\u001b[0m \u001b[43m    \u001b[49m\u001b[43mas_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2244\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2245\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2246\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2247\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2248\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.11/site-packages/pandas/core/groupby/groupby.py:1329\u001b[0m, in \u001b[0;36mGroupBy.__init__\u001b[0;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropna \u001b[38;5;241m=\u001b[39m dropna\n\u001b[1;32m   1328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m grouper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1329\u001b[0m     grouper, exclusions, obj \u001b[38;5;241m=\u001b[39m \u001b[43mget_grouper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1332\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1333\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1334\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mno_default\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1336\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1337\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m observed \u001b[38;5;129;01mis\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n\u001b[1;32m   1340\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ping\u001b[38;5;241m.\u001b[39m_passed_categorical \u001b[38;5;28;01mfor\u001b[39;00m ping \u001b[38;5;129;01min\u001b[39;00m grouper\u001b[38;5;241m.\u001b[39mgroupings):\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.11/site-packages/pandas/core/groupby/grouper.py:1043\u001b[0m, in \u001b[0;36mget_grouper\u001b[0;34m(obj, key, axis, level, sort, observed, validate, dropna)\u001b[0m\n\u001b[1;32m   1041\u001b[0m         in_axis, level, gpr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, gpr, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1043\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(gpr)\n\u001b[1;32m   1044\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(gpr, Grouper) \u001b[38;5;129;01mand\u001b[39;00m gpr\u001b[38;5;241m.\u001b[39mkey \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1045\u001b[0m     \u001b[38;5;66;03m# Add key to exclusions\u001b[39;00m\n\u001b[1;32m   1046\u001b[0m     exclusions\u001b[38;5;241m.\u001b[39madd(gpr\u001b[38;5;241m.\u001b[39mkey)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'surveyId'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "original_train_df = pd.read_csv(\"data/GLC24_PA_metadata_train.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.read_pickle(\"processed_data/internal/test_X.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>surveyId</th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "      <th>year</th>\n",
       "      <th>geoUncertaintyInM</th>\n",
       "      <th>areaInM2</th>\n",
       "      <th>Bio1</th>\n",
       "      <th>Bio2</th>\n",
       "      <th>Bio3</th>\n",
       "      <th>Bio4</th>\n",
       "      <th>...</th>\n",
       "      <th>country_Slovakia</th>\n",
       "      <th>country_Slovenia</th>\n",
       "      <th>country_Spain</th>\n",
       "      <th>country_Switzerland</th>\n",
       "      <th>region_ALPINE</th>\n",
       "      <th>region_ATLANTIC</th>\n",
       "      <th>region_BLACK SEA</th>\n",
       "      <th>region_CONTINENTAL</th>\n",
       "      <th>region_MEDITERRANEAN</th>\n",
       "      <th>region_PANNONIAN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>970</td>\n",
       "      <td>-2.758823</td>\n",
       "      <td>43.068574</td>\n",
       "      <td>2017</td>\n",
       "      <td>5.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2858</td>\n",
       "      <td>79</td>\n",
       "      <td>3</td>\n",
       "      <td>4366</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1650</td>\n",
       "      <td>3.031500</td>\n",
       "      <td>42.749162</td>\n",
       "      <td>2018</td>\n",
       "      <td>10.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2890</td>\n",
       "      <td>76</td>\n",
       "      <td>3</td>\n",
       "      <td>5810</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1741</td>\n",
       "      <td>4.991745</td>\n",
       "      <td>43.433207</td>\n",
       "      <td>2018</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2886</td>\n",
       "      <td>81</td>\n",
       "      <td>3</td>\n",
       "      <td>6054</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1910</td>\n",
       "      <td>12.048730</td>\n",
       "      <td>54.913660</td>\n",
       "      <td>2018</td>\n",
       "      <td>10.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>2821</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>5721</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2037</td>\n",
       "      <td>9.561940</td>\n",
       "      <td>56.078040</td>\n",
       "      <td>2017</td>\n",
       "      <td>10.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>2810</td>\n",
       "      <td>61</td>\n",
       "      <td>2</td>\n",
       "      <td>5664</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 988 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    surveyId        lon        lat  year  geoUncertaintyInM  areaInM2  Bio1  \\\n",
       "20       970  -2.758823  43.068574  2017                5.0     100.0  2858   \n",
       "32      1650   3.031500  42.749162  2018               10.0      16.0  2890   \n",
       "34      1741   4.991745  43.433207  2018                0.0       1.0  2886   \n",
       "38      1910  12.048730  54.913660  2018               10.0      79.0  2821   \n",
       "43      2037   9.561940  56.078040  2017               10.0      79.0  2810   \n",
       "\n",
       "    Bio2  Bio3  Bio4  ...  country_Slovakia  country_Slovenia  country_Spain  \\\n",
       "20    79     3  4366  ...             False             False           True   \n",
       "32    76     3  5810  ...             False             False          False   \n",
       "34    81     3  6054  ...             False             False          False   \n",
       "38    25     1  5721  ...             False             False          False   \n",
       "43    61     2  5664  ...             False             False          False   \n",
       "\n",
       "    country_Switzerland  region_ALPINE  region_ATLANTIC  region_BLACK SEA  \\\n",
       "20                False          False             True             False   \n",
       "32                False          False            False             False   \n",
       "34                False          False            False             False   \n",
       "38                False          False            False             False   \n",
       "43                False          False            False             False   \n",
       "\n",
       "    region_CONTINENTAL  region_MEDITERRANEAN  region_PANNONIAN  \n",
       "20               False                 False             False  \n",
       "32               False                  True             False  \n",
       "34               False                  True             False  \n",
       "38                True                 False             False  \n",
       "43                True                 False             False  \n",
       "\n",
       "[5 rows x 988 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by surveyId and get the number of unique species\n",
    "species_per_survey = original_train_df.groupby(\"surveyId\")[\"speciesId\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "species_per_survey.mean()=16.114713385101194\n",
      "species_per_survey.std()=9.5728748131864\n",
      "species_per_survey.max()=100\n",
      "species_per_survey.min()=1\n"
     ]
    }
   ],
   "source": [
    "print(f\"{species_per_survey.mean()=}\")\n",
    "print(f\"{species_per_survey.std()=}\")\n",
    "print(f\"{species_per_survey.max()=}\")\n",
    "print(f\"{species_per_survey.min()=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "      <th>year</th>\n",
       "      <th>geoUncertaintyInM</th>\n",
       "      <th>areaInM2</th>\n",
       "      <th>region</th>\n",
       "      <th>country</th>\n",
       "      <th>speciesId</th>\n",
       "      <th>surveyId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.099038</td>\n",
       "      <td>43.134956</td>\n",
       "      <td>2021</td>\n",
       "      <td>5.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>MEDITERRANEAN</td>\n",
       "      <td>France</td>\n",
       "      <td>6874.0</td>\n",
       "      <td>212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.099038</td>\n",
       "      <td>43.134956</td>\n",
       "      <td>2021</td>\n",
       "      <td>5.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>MEDITERRANEAN</td>\n",
       "      <td>France</td>\n",
       "      <td>476.0</td>\n",
       "      <td>212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.099038</td>\n",
       "      <td>43.134956</td>\n",
       "      <td>2021</td>\n",
       "      <td>5.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>MEDITERRANEAN</td>\n",
       "      <td>France</td>\n",
       "      <td>11157.0</td>\n",
       "      <td>212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.099038</td>\n",
       "      <td>43.134956</td>\n",
       "      <td>2021</td>\n",
       "      <td>5.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>MEDITERRANEAN</td>\n",
       "      <td>France</td>\n",
       "      <td>8784.0</td>\n",
       "      <td>212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.099038</td>\n",
       "      <td>43.134956</td>\n",
       "      <td>2021</td>\n",
       "      <td>5.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>MEDITERRANEAN</td>\n",
       "      <td>France</td>\n",
       "      <td>4530.0</td>\n",
       "      <td>212</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        lon        lat  year  geoUncertaintyInM  areaInM2         region  \\\n",
       "0  3.099038  43.134956  2021                5.0     100.0  MEDITERRANEAN   \n",
       "1  3.099038  43.134956  2021                5.0     100.0  MEDITERRANEAN   \n",
       "2  3.099038  43.134956  2021                5.0     100.0  MEDITERRANEAN   \n",
       "3  3.099038  43.134956  2021                5.0     100.0  MEDITERRANEAN   \n",
       "4  3.099038  43.134956  2021                5.0     100.0  MEDITERRANEAN   \n",
       "\n",
       "  country  speciesId  surveyId  \n",
       "0  France     6874.0       212  \n",
       "1  France      476.0       212  \n",
       "2  France    11157.0       212  \n",
       "3  France     8784.0       212  \n",
       "4  France     4530.0       212  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data\n",
      "\n",
      "Load data\n",
      "Drop countries not in test data\n",
      "Replace -inf, inf with NaN\n",
      "Process NaN values\n",
      " - Processing train data: areaInM2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:01<00:00, 14.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Processing train data: geoUncertaintyInM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:01<00:00, 14.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Processing test data: areaInM2\n",
      " - Processing test data: geoUncertaintyInM\n",
      "Resulting dataframes\n",
      " - Train data: (1460051, 9)\n",
      " - Test data: (4716, 8)\n",
      "Set speciesId as int\n",
      "Resulting dataframes\n",
      " - Train data: (1460051, 9)\n",
      " - Test data: (4716, 8)\n",
      "Combine environmental data\n",
      " - Processing train data: data/EnvironmentalRasters/EnvironmentalRasters/Climate/Average 1981-2010/GLC24-PA-train-bioclimatic.csv\n",
      " - Processing train data: data/EnvironmentalRasters/EnvironmentalRasters/Climate/Monthly/GLC24-PA-train-bioclimatic_monthly.csv\n",
      " - Processing train data: data/EnvironmentalRasters/EnvironmentalRasters/Elevation/GLC24-PA-train-elevation.csv\n",
      " - Processing train data: data/EnvironmentalRasters/EnvironmentalRasters/Human Footprint/GLC24-PA-train-human_footprint.csv\n",
      " - Processing train data: data/EnvironmentalRasters/EnvironmentalRasters/LandCover/GLC24-PA-train-landcover.csv\n",
      " - Processing train data: data/EnvironmentalRasters/EnvironmentalRasters/SoilGrids/GLC24-PA-train-soilgrids.csv\n",
      " - Processing test data: data/EnvironmentalRasters/EnvironmentalRasters/Climate/Average 1981-2010/GLC24-PA-test-bioclimatic.csv\n",
      " - Processing test data: data/EnvironmentalRasters/EnvironmentalRasters/Climate/Monthly/GLC24-PA-test-bioclimatic_monthly.csv\n",
      " - Processing test data: data/EnvironmentalRasters/EnvironmentalRasters/Elevation/GLC24-PA-test-elevation.csv\n",
      " - Processing test data: data/EnvironmentalRasters/EnvironmentalRasters/Human Footprint/GLC24-PA-test-human_footprint.csv\n",
      " - Processing test data: data/EnvironmentalRasters/EnvironmentalRasters/LandCover/GLC24-PA-test-landcover.csv\n",
      " - Processing test data: data/EnvironmentalRasters/EnvironmentalRasters/SoilGrids/GLC24-PA-test-soilgrids.csv\n",
      "Resulting dataframes\n",
      " - Train data: (1460051, 967)\n",
      " - Test data: (4716, 966)\n",
      "Handle missing data\n",
      " - Processing train data: Elevation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:09<00:00,  1.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Processing train data: HumanFootprint-NavWater1994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:09<00:00,  1.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Processing train data: HumanFootprint-NavWater2009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:09<00:00,  1.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Processing train data: HumanFootprint-Roads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:09<00:00,  1.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Processing train data: HumanFootprint-HFP1993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:09<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Processing train data: HumanFootprint-HFP2009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:09<00:00,  1.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Processing train data: Soilgrid-bdod\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:09<00:00,  1.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Processing train data: Soilgrid-cec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:09<00:00,  1.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Processing train data: Soilgrid-cfvo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:09<00:00,  1.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Processing train data: Soilgrid-clay\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:09<00:00,  1.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Processing train data: Soilgrid-nitrogen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:09<00:00,  1.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Processing train data: Soilgrid-phh2o\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:09<00:00,  1.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Processing train data: Soilgrid-sand\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:09<00:00,  1.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Processing train data: Soilgrid-silt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:09<00:00,  1.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Processing train data: Soilgrid-soc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:09<00:00,  1.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Processing train data: Soilgrid-bdod\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:05<00:00,  3.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Processing train data: Soilgrid-soc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:05<00:00,  3.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Processing train data: Soilgrid-cec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:05<00:00,  3.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Processing train data: Soilgrid-cfvo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:05<00:00,  3.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Processing train data: Soilgrid-clay\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:05<00:00,  3.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Processing train data: Soilgrid-nitrogen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:05<00:00,  3.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Processing train data: Soilgrid-phh2o\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:05<00:00,  3.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Processing train data: Soilgrid-sand\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:05<00:00,  3.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Processing train data: Soilgrid-silt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:05<00:00,  3.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Processing train data: HumanFootprint-NavWater1994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:05<00:00,  3.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Processing train data: HumanFootprint-NavWater2009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:05<00:00,  3.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Processing train data: HumanFootprint-Roads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:05<00:00,  3.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Processing train data: HumanFootprint-HFP1993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:05<00:00,  3.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Processing train data: HumanFootprint-HFP2009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:05<00:00,  3.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resulting dataframes\n",
      " - Train data: (1460051, 967)\n",
      " - Test data: (4716, 966)\n",
      "combined_train_df.index=RangeIndex(start=0, stop=1460051, step=1)\n",
      "combined_test_df.index=RangeIndex(start=0, stop=4716, step=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Data Processor\n",
    "#\n",
    "# Desired output\n",
    "# - Full train dataset - for training model for Kaggle submission\n",
    "# - Full test dataset - for Kaggle submission\n",
    "# - Internal train dataset - 80% of full train dataset for internal banchmarking\n",
    "# - Internal test dataset - 20% of full train dataset for internal benchmarking\n",
    "# - All tabular data in one file\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "print(\"Processing data\\n\")\n",
    "\n",
    "# Load data\n",
    "print(\"Load data\")\n",
    "original_train_df = pd.read_csv(\"data/GLC24_PA_metadata_train.csv\")\n",
    "original_test_df = pd.read_csv(\"data/GLC24_PA_metadata_test.csv\")\n",
    "metadata_train_df = original_train_df.copy()\n",
    "metadata_test_df = original_test_df.copy()\n",
    "\n",
    "# Drop: 'Andorra', 'Hungary', 'Ireland', 'Latvia', 'Luxembourg', 'Monaco', 'Norway',\n",
    "# 'Portugal', 'Romania', 'Serbia', 'The former Yugoslav Republic of Macedonia' they\n",
    "# are not represented in the test set\n",
    "print(\"Drop countries not in test data\")\n",
    "drop_countries = {\n",
    "    \"Andorra\",\n",
    "    \"Hungary\",\n",
    "    \"Ireland\",\n",
    "    \"Latvia\",\n",
    "    \"Luxembourg\",\n",
    "    \"Monaco\",\n",
    "    \"Norway\",\n",
    "    \"Portugal\",\n",
    "    \"Romania\",\n",
    "    \"Serbia\",\n",
    "    \"The former Yugoslav Republic of Macedonia\",\n",
    "}\n",
    "metadata_train_df = metadata_train_df[~metadata_train_df.country.isin(drop_countries)]\n",
    "\n",
    "# -inf, inf replaced by NaN\n",
    "print(\"Replace -inf, inf with NaN\")\n",
    "metadata_train_df = metadata_train_df.replace([np.inf, -np.inf], np.nan)\n",
    "metadata_test_df = metadata_test_df.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# For the train data NaN in geoUncertaintyInM, and areaInM2 replaced with the country median values\n",
    "print(\"Process NaN values\")\n",
    "for column in [\"areaInM2\", \"geoUncertaintyInM\"]:\n",
    "    print(f\" - Processing train data: {column}\")\n",
    "    for country in tqdm(metadata_train_df.country.unique()):\n",
    "        metadata_train_df.loc[\n",
    "            metadata_train_df.country == country, column\n",
    "        ] = metadata_train_df.query(f\"country == '{country}'\")[column].fillna(\n",
    "            metadata_train_df.query(f\"country == '{country}'\")[column].median()\n",
    "        )\n",
    "\n",
    "# For the test data NaN in geoUncertaintyInM, and areaInM2 replaced with the training data country\n",
    "# median values (note this is data leakage, but I believe it is tolerable)\n",
    "for column in [\"areaInM2\", \"geoUncertaintyInM\"]:\n",
    "    print(f\" - Processing test data: {column}\")\n",
    "    for country in metadata_test_df.country.unique():\n",
    "        metadata_test_df.loc[\n",
    "            metadata_test_df.country == country, column\n",
    "        ] = metadata_test_df.query(f\"country == '{country}'\")[column].fillna(\n",
    "            metadata_train_df.query(f\"country == '{country}'\")[column].median()\n",
    "        )\n",
    "\n",
    "# Resulting dataframes\n",
    "print(\"Resulting dataframes\")\n",
    "print(f\" - Train data: {metadata_train_df.shape}\")\n",
    "print(f\" - Test data: {metadata_test_df.shape}\")\n",
    "\n",
    "# set speciesId as int\n",
    "print(\"Set speciesId as int\")\n",
    "metadata_train_df[\"speciesId\"] = metadata_train_df[\"speciesId\"].astype(int)\n",
    "\n",
    "# Resulting dataframes\n",
    "print(\"Resulting dataframes\")\n",
    "print(f\" - Train data: {metadata_train_df.shape}\")\n",
    "print(f\" - Test data: {metadata_test_df.shape}\")\n",
    "\n",
    "# Combine all environmental data\n",
    "print(\"Combine environmental data\")\n",
    "files_to_combine = [\n",
    "    \"data/EnvironmentalRasters/EnvironmentalRasters/Climate/Average 1981-2010/GLC24-PA-{}-bioclimatic.csv\",\n",
    "    \"data/EnvironmentalRasters/EnvironmentalRasters/Climate/Monthly/GLC24-PA-{}-bioclimatic_monthly.csv\",\n",
    "    \"data/EnvironmentalRasters/EnvironmentalRasters/Elevation/GLC24-PA-{}-elevation.csv\",\n",
    "    \"data/EnvironmentalRasters/EnvironmentalRasters/Human Footprint/GLC24-PA-{}-human_footprint.csv\",\n",
    "    \"data/EnvironmentalRasters/EnvironmentalRasters/LandCover/GLC24-PA-{}-landcover.csv\",\n",
    "    \"data/EnvironmentalRasters/EnvironmentalRasters/SoilGrids/GLC24-PA-{}-soilgrids.csv\",\n",
    "]\n",
    "combined_train_df = metadata_train_df\n",
    "combined_test_df = metadata_test_df\n",
    "\n",
    "for file in files_to_combine:\n",
    "    print(\" - Processing train data:\", file.format(\"train\"))\n",
    "    combined_train_df = pd.merge(\n",
    "        combined_train_df, pd.read_csv(file.format(\"train\")), on=\"surveyId\"\n",
    "    )\n",
    "\n",
    "for file in files_to_combine:\n",
    "    print(\" - Processing test data:\", file.format(\"test\"))\n",
    "    combined_test_df = pd.merge(\n",
    "        combined_test_df, pd.read_csv(file.format(\"test\")), on=\"surveyId\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Resulting dataframes\n",
    "print(\"Resulting dataframes\")\n",
    "print(f\" - Train data: {combined_train_df.shape}\")\n",
    "print(f\" - Test data: {combined_test_df.shape}\")\n",
    "\n",
    "# Handle missing data\n",
    "print(\"Handle missing data\")\n",
    "for column in list(\n",
    "    combined_train_df.isna()\n",
    "    .sum()[combined_train_df.isna().sum() > 0]\n",
    "    .keys()\n",
    "):\n",
    "    print(f\" - Processing train data: {column}\")\n",
    "    for country in tqdm(combined_train_df.country.unique()):\n",
    "        combined_train_df.loc[\n",
    "            combined_train_df.country == country, column\n",
    "        ] = combined_train_df.query(f\"country == '{country}'\")[column].fillna(\n",
    "            combined_train_df.query(f\"country == '{country}'\")[column].median()\n",
    "        )\n",
    "\n",
    "for column in list(\n",
    "    combined_train_df.isna()\n",
    "    .sum()[combined_train_df.isna().sum() > 0]\n",
    "    .keys()\n",
    "):\n",
    "    print(f\" - Processing train data: {column}\")\n",
    "    for country in tqdm(combined_train_df.country.unique()):\n",
    "        combined_train_df.loc[\n",
    "            combined_train_df.country == country, column\n",
    "        ] = combined_train_df.query(f\"country == '{country}'\")[column].fillna(\n",
    "            combined_train_df.query(f\"country == '{country}'\")[column].median()\n",
    "        )\n",
    "\n",
    "for column in list(\n",
    "    combined_test_df.isna()\n",
    "    .sum()[combined_test_df.isna().sum() > 0]\n",
    "    .sort_values(ascending=False)\n",
    "    .keys()\n",
    "):\n",
    "    print(f\" - Processing train data: {column}\")\n",
    "    for country in tqdm(combined_test_df.country.unique()):\n",
    "        combined_test_df.loc[\n",
    "            combined_test_df.country == country, column\n",
    "        ] = combined_test_df.query(f\"country == '{country}'\")[column].fillna(\n",
    "            combined_train_df.query(f\"country == '{country}'\")[column].median()\n",
    "        )\n",
    "# Resulting dataframes\n",
    "print(\"Resulting dataframes\")\n",
    "print(f\" - Train data: {combined_train_df.shape}\")\n",
    "print(f\" - Test data: {combined_test_df.shape}\")\n",
    "\n",
    "print(f\"{combined_train_df.index=}\")\n",
    "print(f\"{combined_test_df.index=}\")\n",
    "\n",
    "# for training data country, region, and speciesId one-hot encoded\n",
    "print(\"One-hot encode country, region, and speciesId in training data\")\n",
    "for column in [\"country\", \"region\", \"speciesId\"]:\n",
    "    print(f\" - Processing: {column}\")\n",
    "    ohe_train_df = pd.concat(\n",
    "        [combined_train_df, pd.get_dummies(combined_train_df[column], prefix=column)],\n",
    "        axis=1,\n",
    "    )\n",
    "    ohe_train_df = ohe_train_df.drop(columns=[column])\n",
    "\n",
    "# for test data country and region one-hot encoded\n",
    "print(\"One-hot encode country and region in test data\")\n",
    "for column in [\"country\", \"region\"]:\n",
    "    print(f\" - Processing: {column}\")\n",
    "    ohe_test_df = pd.concat(\n",
    "        [combined_test_df, pd.get_dummies(combined_test_df[column], prefix=column)],\n",
    "        axis=1,\n",
    "    )\n",
    "    ohe_test_df = ohe_test_df.drop(columns=[column])\n",
    "\n",
    "\n",
    "# Grouped by surveyId, use max\n",
    "print(\"Group by surveyId\")\n",
    "grouped_train_df = ohe_train_df.groupby(\"surveyId\", as_index=False).max()\n",
    "\n",
    "# Resulting dataframes\n",
    "print(\"Resulting dataframes\")\n",
    "print(f\" - Train data: {grouped_train_df.shape}\")\n",
    "print(f\" - Test data: {ohe_test_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original_train_df.shape=(1483637, 9)\n",
      "original_test_df.shape=(4716, 8)\n",
      "metadata_train_df.shape=(1460051, 9)\n",
      "metadata_test_df.shape=(4716, 8)\n",
      "combined_train_df.shape=(1460051, 967)\n",
      "combined_test_df.shape=(4716, 966)\n",
      "ohe_train_df.shape=(1460051, 5893)\n",
      "ohe_test_df.shape=(4716, 969)\n",
      "grouped_train_df.shape=(88428, 5892)\n"
     ]
    }
   ],
   "source": [
    "print(f\"{original_train_df.shape=}\")\n",
    "print(f\"{original_test_df.shape=}\")\n",
    "print(f\"{metadata_train_df.shape=}\")\n",
    "print(f\"{metadata_test_df.shape=}\")\n",
    "print(f\"{combined_train_df.shape=}\")\n",
    "print(f\"{combined_test_df.shape=}\")\n",
    "print(f\"{ohe_train_df.shape=}\")\n",
    "print(f\"{ohe_test_df.shape=}\")\n",
    "print(f\"{grouped_train_df.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Find NaN in grouped_train_df\n"
     ]
    }
   ],
   "source": [
    "# find nan in grouped_train_df\n",
    "print(\"Find NaN in grouped_train_df\")\n",
    "for column in ohe_test_df.columns:\n",
    "    if ohe_test_df[column].isna().sum() > 0:\n",
    "        print(f\" - {column}: {ohe_test_df[column].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([    212,     222,     243,     324,     333,     391,     410,     489,\n",
       "           590,     607,\n",
       "       ...\n",
       "       3919341, 3919365, 3919375, 3919517, 3919518, 3919553, 3919592, 3919620,\n",
       "       3919640, 3919655],\n",
       "      dtype='int64', name='surveyId', length=88428)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_train_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "speciesId\n",
       "540      21478\n",
       "4397     19441\n",
       "254      18113\n",
       "4499     15065\n",
       "10317    14538\n",
       "         ...  \n",
       "5806         1\n",
       "8272         1\n",
       "416          1\n",
       "3907         1\n",
       "8119         1\n",
       "Name: count, Length: 4927, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_train_df.speciesId.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RangeIndex(start=0, stop=4716, step=1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohe_test_df.index"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
