{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17159bbd",
   "metadata": {
    "papermill": {
     "duration": 0.006704,
     "end_time": "2024-05-05T14:42:28.165291",
     "exception": false,
     "start_time": "2024-05-05T14:42:28.158587",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# This is an INFERENCE using the model combine the method of MLP, CNN and Transformer\n",
    "\n",
    "Thanks PICEKL for his baseline, which help me a lot.\n",
    "\n",
    "The main idea of this method is to obtain multiple features from multiple dimensions. We simply divide it into four types: **image information** with spatial characteristics, **meta information** that is independent of each other, **climate information** and **satellite information** with time characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1703a0e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T21:25:07.29831Z",
     "start_time": "2024-04-30T21:25:05.354584Z"
    },
    "execution": {
     "iopub.execute_input": "2024-05-05T14:42:41.363659Z",
     "iopub.status.busy": "2024-05-05T14:42:41.363369Z",
     "iopub.status.idle": "2024-05-05T14:42:49.299669Z",
     "shell.execute_reply": "2024-05-05T14:42:49.298878Z"
    },
    "papermill": {
     "duration": 7.946835,
     "end_time": "2024-05-05T14:42:49.302146",
     "exception": false,
     "start_time": "2024-05-05T14:42:41.355311",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange, repeat\n",
    "from pathlib import Path\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1faf2acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dir = Path(\"output\") / time.strftime('%Y-%m-%d_%H%M', time.localtime())\n",
    "new_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c275e9a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-05T14:42:49.317022Z",
     "iopub.status.busy": "2024-05-05T14:42:49.316581Z",
     "iopub.status.idle": "2024-05-05T14:42:49.320755Z",
     "shell.execute_reply": "2024-05-05T14:42:49.319956Z"
    },
    "papermill": {
     "duration": 0.013539,
     "end_time": "2024-05-05T14:42:49.322641",
     "exception": false,
     "start_time": "2024-05-05T14:42:49.309102",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_classes = 11255\n",
    "num_epochs = 100\n",
    "seed = 113"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f91a65",
   "metadata": {
    "papermill": {
     "duration": 0.006362,
     "end_time": "2024-05-05T14:42:49.335533",
     "exception": false,
     "start_time": "2024-05-05T14:42:49.329171",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "When reading data, different **fusion** and **normalization** will be performed for different types of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccd14039",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x750d98237cf0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9782ea2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, metadata, subset, transform=None):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "        self.metadata = metadata\n",
    "\n",
    "        labels = self.metadata[['surveyId' ,'speciesId']].astype(int).copy()\n",
    "        self.label_dict = labels.groupby('surveyId')['speciesId'].apply(list).to_dict()\n",
    "\n",
    "        self.metadata = self.metadata.drop_duplicates(subset=\"surveyId\").reset_index(drop=True)\n",
    "        self.metadata.fillna(0,inplace=True)\n",
    "        self.metadata.replace({float('-inf'): 0}, inplace=True)\n",
    "        self.metadata_data = self.Norm(self.metadata.iloc[:,:5])\n",
    "\n",
    "        self.merge_key = 'surveyId'\n",
    "        self.climate_average = pd.read_csv(\"data/geolifeclef-2024/EnvironmentalRasters/EnvironmentalRasters/Climate/Average 1981-2010/GLC24-PA-train-bioclimatic.csv\")\n",
    "        self.climate_monthly = pd.read_csv(\"data/geolifeclef-2024/EnvironmentalRasters/EnvironmentalRasters/Climate/Monthly/GLC24-PA-train-bioclimatic_monthly.csv\")\n",
    "        self.climate = pd.merge(self.climate_average, self.climate_monthly, on=self.merge_key)\n",
    "        self.climate.fillna(self.climate.mean(),inplace=True)\n",
    "        self.climate_data = self.Norm_all(self.climate)\n",
    "\n",
    "        self.landsat_b = pd.read_csv(\"data/geolifeclef-2024/PA-train-landsat_time_series/GLC24-PA-train-landsat_time_series-blue.csv\")\n",
    "        self.landsat_b.fillna(self.landsat_b.mean(),inplace=True)\n",
    "        self.landsat_g = pd.read_csv(\"data/geolifeclef-2024/PA-train-landsat_time_series/GLC24-PA-train-landsat_time_series-green.csv\")\n",
    "        self.landsat_g.fillna(self.landsat_g.mean(),inplace=True)\n",
    "        self.landsat_r = pd.read_csv(\"data/geolifeclef-2024/PA-train-landsat_time_series/GLC24-PA-train-landsat_time_series-red.csv\")\n",
    "        self.landsat_r.fillna(self.landsat_r.mean(),inplace=True)\n",
    "        self.landsat_n = pd.read_csv(\"data/geolifeclef-2024/PA-train-landsat_time_series/GLC24-PA-train-landsat_time_series-nir.csv\")\n",
    "        self.landsat_n.fillna(self.landsat_n.mean(),inplace=True)\n",
    "        self.landsat_s1 = pd.read_csv(\"data/geolifeclef-2024/PA-train-landsat_time_series/GLC24-PA-train-landsat_time_series-swir1.csv\")\n",
    "        self.landsat_s1.fillna(self.landsat_s1.mean(),inplace=True)\n",
    "        self.landsat_s2 = pd.read_csv(\"data/geolifeclef-2024/PA-train-landsat_time_series/GLC24-PA-train-landsat_time_series-swir2.csv\")\n",
    "        self.landsat_s2.fillna(self.landsat_s2.mean(),inplace=True)\n",
    "        self.landsat_data = torch.cat([self.Norm_all(self.landsat_b),self.Norm_all(self.landsat_g),self.Norm_all(self.landsat_r),self.Norm_all(self.landsat_n),self.Norm_all(self.landsat_s1),self.Norm_all(self.landsat_s2)],axis=1)\n",
    "\n",
    "        self.elevation = pd.read_csv(\"data/geolifeclef-2024/EnvironmentalRasters/EnvironmentalRasters/Elevation/GLC24-PA-train-elevation.csv\")\n",
    "        self.elevation[self.elevation<0]=0\n",
    "        self.elevation.fillna(self.elevation.mean(),inplace=True)\n",
    "        self.elevation_data = self.Norm(self.elevation)\n",
    "\n",
    "        self.human_footprint = pd.read_csv(\"data/geolifeclef-2024/EnvironmentalRasters/EnvironmentalRasters/Human Footprint/GLC24-PA-train-human_footprint.csv\")\n",
    "        self.human_footprint[self.human_footprint<0]=0\n",
    "        self.human_footprint.fillna(self.human_footprint.mean(),inplace=True)\n",
    "        self.human_footprint_data = self.Norm(self.human_footprint)\n",
    "\n",
    "        self.landcover = pd.read_csv(\"data/geolifeclef-2024/EnvironmentalRasters/EnvironmentalRasters/LandCover/GLC24-PA-train-landcover.csv\")\n",
    "        self.landcover[self.landcover<0]=0\n",
    "        self.landcover.fillna(self.landcover.mean(),inplace=True)\n",
    "        self.landcover_data = self.Norm(self.landcover)\n",
    "\n",
    "        self.soilgrids = pd.read_csv(\"data/geolifeclef-2024/EnvironmentalRasters/EnvironmentalRasters/SoilGrids/GLC24-PA-train-soilgrids.csv\")\n",
    "        self.soilgrids[self.soilgrids<0]=0\n",
    "        self.soilgrids.fillna(self.soilgrids.mean(),inplace=True)\n",
    "        self.soilgrids_data = self.Norm(self.soilgrids)\n",
    "\n",
    "        self.metadata_data = torch.cat((self.metadata_data, self.elevation_data, self.human_footprint_data, self.landcover_data, self.soilgrids_data), dim=1)\n",
    "\n",
    "    def Norm(self,df):\n",
    "        output=torch.from_numpy(df.iloc[:,1:].values).float()\n",
    "        return (output-output.mean(dim=0))/output.std(dim=0)\n",
    "\n",
    "    def Norm_all(self,df):\n",
    "        output=torch.from_numpy(df.iloc[:,1:].values).float()\n",
    "        return (output-output.mean())/output.std()\n",
    "\n",
    "    def patch_rgb_path(self,survey_id):\n",
    "        path = \"data/geolifeclef-2024/PA_Train_SatellitePatches_RGB/pa_train_patches_rgb\"\n",
    "        for d in (str(survey_id)[-2:], str(survey_id)[-4:-2]):\n",
    "            path = os.path.join(path, d)\n",
    "        path = os.path.join(path, f\"{survey_id}.jpeg\")\n",
    "        return path\n",
    "\n",
    "    def patch_nir_path(self,survey_id):\n",
    "        path = \"data/geolifeclef-2024/PA_Train_SatellitePatches_NIR/pa_train_patches_nir\"\n",
    "        for d in (str(survey_id)[-2:], str(survey_id)[-4:-2]):\n",
    "            path = os.path.join(path, d)\n",
    "        path = os.path.join(path, f\"{survey_id}.jpeg\")\n",
    "        return path\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        survey_id = self.metadata.surveyId[idx]\n",
    "\n",
    "        image_path = self.patch_rgb_path(survey_id)\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        image = self.transform(image)\n",
    "        image = image.unsqueeze(0)\n",
    "        image_nir_path = self.patch_nir_path(survey_id)\n",
    "        nir_image = Image.open(image_nir_path).convert(\"L\")\n",
    "        nir_image = self.transform(nir_image)\n",
    "        nir_image = nir_image.unsqueeze(0)\n",
    "        image_data = torch.cat([image,nir_image],dim=1)\n",
    "        image_data = torch.squeeze(image_data)\n",
    "        sample=[self.metadata_data[idx,:],image_data,self.landsat_data[idx,:],self.climate_data[idx,:]]\n",
    "        species_ids = self.label_dict[survey_id]  # Get list of species IDs for the survey ID\n",
    "        label = torch.zeros(num_classes)  # Initialize label tensor\n",
    "        for species_id in species_ids:\n",
    "            label[species_id] = 1  # Set the corresponding class index to 1 for each species ID\n",
    "        count = len(species_ids)\n",
    "        return sample, survey_id, label, count#, species_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af853980",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T21:25:32.627928Z",
     "start_time": "2024-04-30T21:25:32.612131Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-05-05T14:42:49.350022Z",
     "iopub.status.busy": "2024-05-05T14:42:49.349757Z",
     "iopub.status.idle": "2024-05-05T14:42:49.375102Z",
     "shell.execute_reply": "2024-05-05T14:42:49.374208Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.035054,
     "end_time": "2024-05-05T14:42:49.377072",
     "exception": false,
     "start_time": "2024-05-05T14:42:49.342018",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, metadata, subset, transform=None):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "        self.metadata = metadata\n",
    "\n",
    "        self.metadata = self.metadata.drop_duplicates(subset=\"surveyId\").reset_index(drop=True)\n",
    "        self.metadata.fillna(0,inplace=True)\n",
    "        self.metadata.replace({float('-inf'): 0}, inplace=True)\n",
    "        self.metadata_data = self.Norm(self.metadata.iloc[:,:5])\n",
    "\n",
    "        self.merge_key = 'surveyId'\n",
    "        self.climate_average = pd.read_csv(\"data/geolifeclef-2024/EnvironmentalRasters/EnvironmentalRasters/Climate/Average 1981-2010/GLC24-PA-test-bioclimatic.csv\")\n",
    "        self.climate_monthly = pd.read_csv(\"data/geolifeclef-2024/EnvironmentalRasters/EnvironmentalRasters/Climate/Monthly/GLC24-PA-test-bioclimatic_monthly.csv\")\n",
    "        self.climate = pd.merge(self.climate_average, self.climate_monthly, on=self.merge_key)\n",
    "        self.climate.fillna(self.climate.mean(),inplace=True)\n",
    "        self.climate_data = self.Norm_all(self.climate)\n",
    "\n",
    "        self.landsat_b = pd.read_csv(\"data/geolifeclef-2024/PA-test-landsat_time_series/GLC24-PA-test-landsat_time_series-blue.csv\")\n",
    "        self.landsat_b.fillna(self.landsat_b.mean(),inplace=True)\n",
    "        self.landsat_g = pd.read_csv(\"data/geolifeclef-2024/PA-test-landsat_time_series/GLC24-PA-test-landsat_time_series-green.csv\")\n",
    "        self.landsat_g.fillna(self.landsat_g.mean(),inplace=True)\n",
    "        self.landsat_r = pd.read_csv(\"data/geolifeclef-2024/PA-test-landsat_time_series/GLC24-PA-test-landsat_time_series-red.csv\")\n",
    "        self.landsat_r.fillna(self.landsat_r.mean(),inplace=True)\n",
    "        self.landsat_n = pd.read_csv(\"data/geolifeclef-2024/PA-test-landsat_time_series/GLC24-PA-test-landsat_time_series-nir.csv\")\n",
    "        self.landsat_n.fillna(self.landsat_n.mean(),inplace=True)\n",
    "        self.landsat_s1 = pd.read_csv(\"data/geolifeclef-2024/PA-test-landsat_time_series/GLC24-PA-test-landsat_time_series-swir1.csv\")\n",
    "        self.landsat_s1.fillna(self.landsat_s1.mean(),inplace=True)\n",
    "        self.landsat_s2 = pd.read_csv(\"data/geolifeclef-2024/PA-test-landsat_time_series/GLC24-PA-test-landsat_time_series-swir2.csv\")\n",
    "        self.landsat_s2.fillna(self.landsat_s2.mean(),inplace=True)\n",
    "        self.landsat_data = torch.cat([self.Norm_all(self.landsat_b),self.Norm_all(self.landsat_g),self.Norm_all(self.landsat_r),self.Norm_all(self.landsat_n),self.Norm_all(self.landsat_s1),self.Norm_all(self.landsat_s2)],axis=1)\n",
    "\n",
    "        self.elevation = pd.read_csv(\"data/geolifeclef-2024/EnvironmentalRasters/EnvironmentalRasters/Elevation/GLC24-PA-test-elevation.csv\")\n",
    "        self.elevation[self.elevation<0]=0\n",
    "        self.elevation.fillna(self.elevation.mean(),inplace=True)\n",
    "        self.elevation_data = self.Norm(self.elevation)\n",
    "\n",
    "        self.human_footprint = pd.read_csv(\"data/geolifeclef-2024/EnvironmentalRasters/EnvironmentalRasters/Human Footprint/GLC24-PA-test-human_footprint.csv\")\n",
    "        self.human_footprint[self.human_footprint<0]=0\n",
    "        self.human_footprint.fillna(self.human_footprint.mean(),inplace=True)\n",
    "        self.human_footprint_data = self.Norm(self.human_footprint)\n",
    "\n",
    "        self.landcover = pd.read_csv(\"data/geolifeclef-2024/EnvironmentalRasters/EnvironmentalRasters/LandCover/GLC24-PA-test-landcover.csv\")\n",
    "        self.landcover[self.landcover<0]=0\n",
    "        self.landcover.fillna(self.landcover.mean(),inplace=True)\n",
    "        self.landcover_data = self.Norm(self.landcover)\n",
    "\n",
    "        self.soilgrids = pd.read_csv(\"data/geolifeclef-2024/EnvironmentalRasters/EnvironmentalRasters/SoilGrids/GLC24-PA-test-soilgrids.csv\")\n",
    "        self.soilgrids[self.soilgrids<0]=0\n",
    "        self.soilgrids.fillna(self.soilgrids.mean(),inplace=True)\n",
    "        self.soilgrids_data = self.Norm(self.soilgrids)\n",
    "\n",
    "        self.metadata_data = torch.cat((self.metadata_data, self.elevation_data, self.human_footprint_data, self.landcover_data, self.soilgrids_data), dim=1)\n",
    "\n",
    "    def Norm(self,df):\n",
    "        output=torch.from_numpy(df.iloc[:,1:].values).float()\n",
    "        return (output-output.mean(dim=0))/output.std(dim=0)\n",
    "\n",
    "    def Norm_all(self,df):\n",
    "        output=torch.from_numpy(df.iloc[:,1:].values).float()\n",
    "        return (output-output.mean())/output.std()\n",
    "\n",
    "    def patch_rgb_path(self,survey_id):\n",
    "        path = \"data/geolifeclef-2024/PA_Test_SatellitePatches_RGB/pa_test_patches_rgb\"\n",
    "        for d in (str(survey_id)[-2:], str(survey_id)[-4:-2]):\n",
    "            path = os.path.join(path, d)\n",
    "        path = os.path.join(path, f\"{survey_id}.jpeg\")\n",
    "        return path\n",
    "\n",
    "    def patch_nir_path(self,survey_id):\n",
    "        path = \"data/geolifeclef-2024/PA_Test_SatellitePatches_NIR/pa_test_patches_nir\"\n",
    "        for d in (str(survey_id)[-2:], str(survey_id)[-4:-2]):\n",
    "            path = os.path.join(path, d)\n",
    "        path = os.path.join(path, f\"{survey_id}.jpeg\")\n",
    "        return path\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        survey_id = self.metadata.surveyId[idx]\n",
    "\n",
    "        image_path = self.patch_rgb_path(survey_id)\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        image = self.transform(image)\n",
    "        image = image.unsqueeze(0)\n",
    "        image_nir_path = self.patch_nir_path(survey_id)\n",
    "        nir_image = Image.open(image_nir_path).convert(\"L\")\n",
    "        nir_image = self.transform(nir_image)\n",
    "        nir_image = nir_image.unsqueeze(0)\n",
    "        image_data = torch.cat([image,nir_image],dim=1)\n",
    "        image_data = torch.squeeze(image_data)\n",
    "        sample=[self.metadata_data[idx,:],image_data,self.landsat_data[idx,:],self.climate_data[idx,:]]\n",
    "        return sample, survey_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39b64182",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T21:25:34.532017Z",
     "start_time": "2024-04-30T21:25:32.615562Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-05-05T14:42:49.391704Z",
     "iopub.status.busy": "2024-05-05T14:42:49.391421Z",
     "iopub.status.idle": "2024-05-05T14:42:51.305244Z",
     "shell.execute_reply": "2024-05-05T14:42:51.304427Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 1.923732,
     "end_time": "2024-05-05T14:42:51.307566",
     "exception": false,
     "start_time": "2024-05-05T14:42:49.383834",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dataset and DataLoader\n",
    "train_batch_size = 64\n",
    "test_batch_size = 1\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128,128)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Load training data\n",
    "train_metadata_path = \"data/geolifeclef-2024/GLC24_PA_metadata_train.csv\"\n",
    "train_metadata = pd.read_csv(train_metadata_path)\n",
    "train_dataset = TrainDataset(train_metadata, subset=\"train\", transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=False, num_workers=1)\n",
    "\n",
    "# Load testing data\n",
    "test_metadata_path = \"data/geolifeclef-2024/GLC24_PA_metadata_test.csv\"\n",
    "test_metadata = pd.read_csv(test_metadata_path)\n",
    "test_dataset = TestDataset(test_metadata, subset=\"test\", transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad70c7af",
   "metadata": {
    "papermill": {
     "duration": 0.006496,
     "end_time": "2024-05-05T14:42:51.321299",
     "exception": false,
     "start_time": "2024-05-05T14:42:51.314803",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This is an MLP used to extract features from generally independent information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7143060",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-05T14:42:51.335647Z",
     "iopub.status.busy": "2024-05-05T14:42:51.335345Z",
     "iopub.status.idle": "2024-05-05T14:42:51.341096Z",
     "shell.execute_reply": "2024-05-05T14:42:51.340279Z"
    },
    "papermill": {
     "duration": 0.015091,
     "end_time": "2024-05-05T14:42:51.342937",
     "exception": false,
     "start_time": "2024-05-05T14:42:51.327846",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(dim, out_dim*5)\n",
    "        self.fc2 = nn.Linear(out_dim*5, out_dim)\n",
    "        self.norm = nn.LayerNorm(out_dim*5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.tanh(self.fc1(x))\n",
    "        x = self.norm(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18c14da",
   "metadata": {
    "papermill": {
     "duration": 0.006331,
     "end_time": "2024-05-05T14:42:51.355818",
     "exception": false,
     "start_time": "2024-05-05T14:42:51.349487",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The following is the part of ViT, which can also be considered as the Encoder part of **Transformer**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c372000",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-05T14:42:51.369938Z",
     "iopub.status.busy": "2024-05-05T14:42:51.369682Z",
     "iopub.status.idle": "2024-05-05T14:42:51.377708Z",
     "shell.execute_reply": "2024-05-05T14:42:51.376886Z"
    },
    "papermill": {
     "duration": 0.017177,
     "end_time": "2024-05-05T14:42:51.379537",
     "exception": false,
     "start_time": "2024-05-05T14:42:51.362360",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Multihead_self_attention(nn.Module):\n",
    "    def __init__(self, heads, head_dim, dim):\n",
    "        super().__init__()\n",
    "        self.head_dim = head_dim\n",
    "        self.heads = heads\n",
    "        self.inner_dim = self.heads*self.head_dim\n",
    "        self.scale = self.head_dim**-0.5\n",
    "        self.to_qkv = nn.Linear(dim, self.inner_dim*3)\n",
    "        self.to_output = nn.Linear(self.inner_dim, dim)\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
    "        Q, K, V = map(lambda t: rearrange(t, 'b l (h dim) -> b h l dim', dim=self.head_dim), qkv)\n",
    "        K_T = K.transpose(-1, -2)\n",
    "        att_score = Q@K_T*self.scale\n",
    "        att = self.softmax(att_score)\n",
    "        out = att@V   # (B,H,L,dim)\n",
    "        out = rearrange(out, 'b h l dim -> b l (h dim)')\n",
    "        output = self.to_output(out)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3b7da87",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-05T14:42:51.394418Z",
     "iopub.status.busy": "2024-05-05T14:42:51.394157Z",
     "iopub.status.idle": "2024-05-05T14:42:51.399714Z",
     "shell.execute_reply": "2024-05-05T14:42:51.398927Z"
    },
    "papermill": {
     "duration": 0.014669,
     "end_time": "2024-05-05T14:42:51.401491",
     "exception": false,
     "start_time": "2024-05-05T14:42:51.386822",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, mlp_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(dim, mlp_dim)\n",
    "        self.fc2 = nn.Linear(mlp_dim, dim)\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        x = F.gelu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38ffea8c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-05T14:42:51.415374Z",
     "iopub.status.busy": "2024-05-05T14:42:51.415124Z",
     "iopub.status.idle": "2024-05-05T14:42:51.420412Z",
     "shell.execute_reply": "2024-05-05T14:42:51.419638Z"
    },
    "papermill": {
     "duration": 0.014302,
     "end_time": "2024-05-05T14:42:51.422268",
     "exception": false,
     "start_time": "2024-05-05T14:42:51.407966",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Transformer_block(nn.Module):\n",
    "    def __init__(self, dim, heads, head_dim, mlp_dim):\n",
    "        super().__init__()\n",
    "        self.MHA = Multihead_self_attention(heads=heads, head_dim=head_dim, dim=dim)\n",
    "        self.FeedForward = FeedForward(dim=dim, mlp_dim=mlp_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.MHA(x)+x\n",
    "        x = self.FeedForward(x)+x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "455d578b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-05T14:42:51.436184Z",
     "iopub.status.busy": "2024-05-05T14:42:51.435904Z",
     "iopub.status.idle": "2024-05-05T14:42:51.441971Z",
     "shell.execute_reply": "2024-05-05T14:42:51.441163Z"
    },
    "papermill": {
     "duration": 0.015236,
     "end_time": "2024-05-05T14:42:51.443905",
     "exception": false,
     "start_time": "2024-05-05T14:42:51.428669",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self, dim, heads, head_dim, mlp_dim, num_class):\n",
    "        super().__init__()\n",
    "        self.transformer = Transformer_block(dim=dim, heads=heads, head_dim=head_dim, mlp_dim=mlp_dim)\n",
    "\n",
    "        self.MLP_head = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, num_class)\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.transformer(x)\n",
    "        CLS_token = x[:, 0, :]\n",
    "        out = self.MLP_head(CLS_token)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7688f26e",
   "metadata": {
    "papermill": {
     "duration": 0.006234,
     "end_time": "2024-05-05T14:42:51.456632",
     "exception": false,
     "start_time": "2024-05-05T14:42:51.450398",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The following is **CNN**, used to extract feature information from images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70f497c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-05T14:42:51.470881Z",
     "iopub.status.busy": "2024-05-05T14:42:51.470190Z",
     "iopub.status.idle": "2024-05-05T14:42:51.476395Z",
     "shell.execute_reply": "2024-05-05T14:42:51.475611Z"
    },
    "papermill": {
     "duration": 0.015063,
     "end_time": "2024-05-05T14:42:51.478141",
     "exception": false,
     "start_time": "2024-05-05T14:42:51.463078",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ResNet18(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ResNet18, self).__init__()\n",
    "\n",
    "        self.resnet18 = models.resnet18(weights=None)\n",
    "        self.resnet18.conv1 = nn.Conv2d(4, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.resnet18.maxpool = nn.Identity()\n",
    "        self.ln = nn.LayerNorm(1000)\n",
    "        self.fc1 = nn.Linear(1000, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet18(x)\n",
    "        x = self.ln(x)\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b334f1d",
   "metadata": {
    "papermill": {
     "duration": 0.006256,
     "end_time": "2024-05-05T14:42:51.490923",
     "exception": false,
     "start_time": "2024-05-05T14:42:51.484667",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This is the **final multi-modal model**. Unlike ViT, its input is features extracted from each dimension. \n",
    "\n",
    "For features with time information, I added additional position information. \n",
    "\n",
    "At the same time, I also added a feature that is a fusion of the first four features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f8dde92",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-05T14:42:51.505245Z",
     "iopub.status.busy": "2024-05-05T14:42:51.504543Z",
     "iopub.status.idle": "2024-05-05T14:42:51.515887Z",
     "shell.execute_reply": "2024-05-05T14:42:51.515235Z"
    },
    "papermill": {
     "duration": 0.020283,
     "end_time": "2024-05-05T14:42:51.517661",
     "exception": false,
     "start_time": "2024-05-05T14:42:51.497378",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiModal(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(MultiModal, self).__init__()\n",
    "        self.cls = nn.Parameter(torch.randn(1, 1, 200))\n",
    "        self.meta = Embedding(31,200)\n",
    "        self.resnet18 = ResNet18(200)\n",
    "        self.landsat = Embedding(504,200)\n",
    "        self.position_landsat = nn.Parameter(torch.randn(1, 504))\n",
    "        self.climate = Embedding(931,200)\n",
    "        self.position_climate = nn.Parameter(torch.randn(1, 931))\n",
    "        self.emb = Embedding(800,200)\n",
    "        self.position_combine = nn.Parameter(torch.randn(1, 800))\n",
    "        self.vit = ViT(200, 2, 200, 400, num_classes)\n",
    "        self.position = nn.Parameter(torch.randn(1, 6, 200))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch = x[0].size(0)\n",
    "        CLS = repeat(self.cls, '1 1 d -> b 1 d', b=batch).to(device)\n",
    "        META = self.meta(x[0])\n",
    "        IMG = self.resnet18(x[1])\n",
    "        LANDSAT = self.landsat(x[2]+self.position_landsat)\n",
    "        CLIMATE = self.climate(x[3]+self.position_climate)\n",
    "        combine = torch.cat((META, IMG, LANDSAT, CLIMATE), dim=1)\n",
    "        COMBINE = self.emb(combine+self.position_combine)\n",
    "        token = torch.concat((CLS, META.unsqueeze(1)), dim=1)\n",
    "        token = torch.concat((token, IMG.unsqueeze(1)), dim=1)\n",
    "        token = torch.concat((token, LANDSAT.unsqueeze(1)), dim=1)\n",
    "        token = torch.concat((token, CLIMATE.unsqueeze(1)), dim=1)\n",
    "        token = torch.concat((token, COMBINE.unsqueeze(1)), dim=1)\n",
    "        out = self.vit(token+self.position)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8e1f40e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T21:25:31.611823Z",
     "start_time": "2024-04-30T21:25:31.607373Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-05-05T14:42:51.531532Z",
     "iopub.status.busy": "2024-05-05T14:42:51.531262Z",
     "iopub.status.idle": "2024-05-05T14:42:52.798635Z",
     "shell.execute_reply": "2024-05-05T14:42:52.797682Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 1.276491,
     "end_time": "2024-05-05T14:42:52.800566",
     "exception": false,
     "start_time": "2024-05-05T14:42:51.524075",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE = CUDA\n"
     ]
    }
   ],
   "source": [
    "# Check if cuda is available\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"DEVICE = CUDA\")\n",
    "\n",
    "model = MultiModal(num_classes).to(device)\n",
    "# model.load_state_dict(torch.load(\"models/vit/Model.pth\", map_location=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ba363e",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c7e9665",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/startung/miniconda3/envs/pt/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.00025)\n",
    "# logger.info(\"Optimizer: AdamW\")\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=25, verbose=True)\n",
    "# logger.info(\"Scheduler: CosineAnnealingLR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0dc6840b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 100 epochs started.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Batch 0/1391, Loss: 0.7349907159805298\n",
      "Epoch 1/100, Batch 139/1391, Loss: 0.029572946950793266\n",
      "Epoch 1/100, Batch 278/1391, Loss: 0.014072494581341743\n",
      "Epoch 1/100, Batch 417/1391, Loss: 0.010163888335227966\n",
      "Epoch 1/100, Batch 556/1391, Loss: 0.00869766902178526\n",
      "Epoch 1/100, Batch 695/1391, Loss: 0.007862644270062447\n",
      "Epoch 1/100, Batch 834/1391, Loss: 0.008331937715411186\n",
      "Epoch 1/100, Batch 973/1391, Loss: 0.007483727764338255\n",
      "Epoch 1/100, Batch 1112/1391, Loss: 0.006611417513340712\n",
      "Epoch 1/100, Batch 1251/1391, Loss: 0.006771528162062168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [04:20<7:09:55, 260.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Batch 1390/1391, Loss: 0.005203625187277794\n",
      "Epoch 2/100, Batch 0/1391, Loss: 0.005556628108024597\n",
      "Epoch 2/100, Batch 139/1391, Loss: 0.005286202300339937\n",
      "Epoch 2/100, Batch 278/1391, Loss: 0.0059546516276896\n",
      "Epoch 2/100, Batch 417/1391, Loss: 0.00588897755369544\n",
      "Epoch 2/100, Batch 556/1391, Loss: 0.005632961168885231\n",
      "Epoch 2/100, Batch 695/1391, Loss: 0.005461188033223152\n",
      "Epoch 2/100, Batch 834/1391, Loss: 0.0060033234767615795\n",
      "Epoch 2/100, Batch 973/1391, Loss: 0.005516052711755037\n",
      "Epoch 2/100, Batch 1112/1391, Loss: 0.0049687596037983894\n",
      "Epoch 2/100, Batch 1251/1391, Loss: 0.005389644298702478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/100 [08:42<7:07:17, 261.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100, Batch 1390/1391, Loss: 0.004094450268894434\n",
      "Epoch 3/100, Batch 0/1391, Loss: 0.0044764368794858456\n",
      "Epoch 3/100, Batch 139/1391, Loss: 0.004302266053855419\n",
      "Epoch 3/100, Batch 278/1391, Loss: 0.004993549082428217\n",
      "Epoch 3/100, Batch 417/1391, Loss: 0.0051199281588196754\n",
      "Epoch 3/100, Batch 556/1391, Loss: 0.004981357604265213\n",
      "Epoch 3/100, Batch 695/1391, Loss: 0.0048621646128594875\n",
      "Epoch 3/100, Batch 834/1391, Loss: 0.005269576329737902\n",
      "Epoch 3/100, Batch 973/1391, Loss: 0.004949171096086502\n",
      "Epoch 3/100, Batch 1112/1391, Loss: 0.004477598238736391\n",
      "Epoch 3/100, Batch 1251/1391, Loss: 0.004935792647302151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3/100 [13:05<7:03:31, 261.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100, Batch 1390/1391, Loss: 0.0037795782554894686\n",
      "Epoch 4/100, Batch 0/1391, Loss: 0.004124871920794249\n",
      "Epoch 4/100, Batch 139/1391, Loss: 0.003960103262215853\n",
      "Epoch 4/100, Batch 278/1391, Loss: 0.004616585560142994\n",
      "Epoch 4/100, Batch 417/1391, Loss: 0.004743389785289764\n",
      "Epoch 4/100, Batch 556/1391, Loss: 0.004715072922408581\n",
      "Epoch 4/100, Batch 695/1391, Loss: 0.004523777402937412\n",
      "Epoch 4/100, Batch 834/1391, Loss: 0.004976877011358738\n",
      "Epoch 4/100, Batch 973/1391, Loss: 0.00461329473182559\n",
      "Epoch 4/100, Batch 1112/1391, Loss: 0.004278188105672598\n",
      "Epoch 4/100, Batch 1251/1391, Loss: 0.004747818224132061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4/100 [17:28<6:59:49, 262.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/100, Batch 1390/1391, Loss: 0.0036199241876602173\n",
      "Epoch 5/100, Batch 0/1391, Loss: 0.003939446993172169\n",
      "Epoch 5/100, Batch 139/1391, Loss: 0.0037916458677500486\n",
      "Epoch 5/100, Batch 278/1391, Loss: 0.004410835914313793\n",
      "Epoch 5/100, Batch 417/1391, Loss: 0.004545021802186966\n",
      "Epoch 5/100, Batch 556/1391, Loss: 0.0045617795549333096\n",
      "Epoch 5/100, Batch 695/1391, Loss: 0.004355668090283871\n",
      "Epoch 5/100, Batch 834/1391, Loss: 0.004785933066159487\n",
      "Epoch 5/100, Batch 973/1391, Loss: 0.004430574364960194\n",
      "Epoch 5/100, Batch 1112/1391, Loss: 0.004096958786249161\n",
      "Epoch 5/100, Batch 1251/1391, Loss: 0.0046019237488508224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5/100 [21:50<6:55:33, 262.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100, Batch 1390/1391, Loss: 0.0035346036311239004\n",
      "Epoch 6/100, Batch 0/1391, Loss: 0.0037472706753760576\n",
      "Epoch 6/100, Batch 139/1391, Loss: 0.0036977191921323538\n",
      "Epoch 6/100, Batch 278/1391, Loss: 0.0042938655242323875\n",
      "Epoch 6/100, Batch 417/1391, Loss: 0.004393143579363823\n",
      "Epoch 6/100, Batch 556/1391, Loss: 0.004423927515745163\n",
      "Epoch 6/100, Batch 695/1391, Loss: 0.0042361607775092125\n",
      "Epoch 6/100, Batch 834/1391, Loss: 0.004649145528674126\n",
      "Epoch 6/100, Batch 973/1391, Loss: 0.004380247090011835\n",
      "Epoch 6/100, Batch 1112/1391, Loss: 0.004043597728013992\n",
      "Epoch 6/100, Batch 1251/1391, Loss: 0.004457805771380663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 6/100 [26:13<6:51:15, 262.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100, Batch 1390/1391, Loss: 0.0034150693099945784\n",
      "Epoch 7/100, Batch 0/1391, Loss: 0.003647738602012396\n",
      "Epoch 7/100, Batch 139/1391, Loss: 0.0036405082792043686\n",
      "Epoch 7/100, Batch 278/1391, Loss: 0.0042239646427333355\n",
      "Epoch 7/100, Batch 417/1391, Loss: 0.0042792572639882565\n",
      "Epoch 7/100, Batch 556/1391, Loss: 0.004339543636888266\n",
      "Epoch 7/100, Batch 695/1391, Loss: 0.004147310741245747\n",
      "Epoch 7/100, Batch 834/1391, Loss: 0.004526773001998663\n",
      "Epoch 7/100, Batch 973/1391, Loss: 0.004238068591803312\n",
      "Epoch 7/100, Batch 1112/1391, Loss: 0.003913720138370991\n",
      "Epoch 7/100, Batch 1251/1391, Loss: 0.004436977207660675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 7/100 [30:36<6:46:54, 262.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/100, Batch 1390/1391, Loss: 0.0033450121991336346\n",
      "Epoch 8/100, Batch 0/1391, Loss: 0.0035453469026833773\n",
      "Epoch 8/100, Batch 139/1391, Loss: 0.003624115139245987\n",
      "Epoch 8/100, Batch 278/1391, Loss: 0.004147933330386877\n",
      "Epoch 8/100, Batch 417/1391, Loss: 0.004192279651761055\n",
      "Epoch 8/100, Batch 556/1391, Loss: 0.0042151776142418385\n",
      "Epoch 8/100, Batch 695/1391, Loss: 0.004130037501454353\n",
      "Epoch 8/100, Batch 834/1391, Loss: 0.004439074080437422\n",
      "Epoch 8/100, Batch 973/1391, Loss: 0.00412951223552227\n",
      "Epoch 8/100, Batch 1112/1391, Loss: 0.003863994497805834\n",
      "Epoch 8/100, Batch 1251/1391, Loss: 0.0043493956327438354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 8/100 [34:58<6:42:33, 262.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/100, Batch 1390/1391, Loss: 0.003260658122599125\n",
      "Epoch 9/100, Batch 0/1391, Loss: 0.003475713077932596\n",
      "Epoch 9/100, Batch 139/1391, Loss: 0.0035386730451136827\n",
      "Epoch 9/100, Batch 278/1391, Loss: 0.004021951928734779\n",
      "Epoch 9/100, Batch 417/1391, Loss: 0.004149140324443579\n",
      "Epoch 9/100, Batch 556/1391, Loss: 0.004132898524403572\n",
      "Epoch 9/100, Batch 695/1391, Loss: 0.004038461484014988\n",
      "Epoch 9/100, Batch 834/1391, Loss: 0.004364408552646637\n",
      "Epoch 9/100, Batch 973/1391, Loss: 0.0040654209442436695\n",
      "Epoch 9/100, Batch 1112/1391, Loss: 0.0038547220174223185\n",
      "Epoch 9/100, Batch 1251/1391, Loss: 0.004263853654265404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 9/100 [39:21<6:38:14, 262.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/100, Batch 1390/1391, Loss: 0.0031729491893202066\n",
      "Epoch 10/100, Batch 0/1391, Loss: 0.0034426318015903234\n",
      "Epoch 10/100, Batch 139/1391, Loss: 0.003508793655782938\n",
      "Epoch 10/100, Batch 278/1391, Loss: 0.003971154801547527\n",
      "Epoch 10/100, Batch 417/1391, Loss: 0.004087402950972319\n",
      "Epoch 10/100, Batch 556/1391, Loss: 0.004039234481751919\n",
      "Epoch 10/100, Batch 695/1391, Loss: 0.003984333481639624\n",
      "Epoch 10/100, Batch 834/1391, Loss: 0.004338400438427925\n",
      "Epoch 10/100, Batch 973/1391, Loss: 0.004050201270729303\n",
      "Epoch 10/100, Batch 1112/1391, Loss: 0.003766178386285901\n",
      "Epoch 10/100, Batch 1251/1391, Loss: 0.004228893201798201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 10/100 [43:43<6:33:52, 262.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100, Batch 1390/1391, Loss: 0.0030899227131158113\n",
      "Epoch 11/100, Batch 0/1391, Loss: 0.003362937830388546\n",
      "Epoch 11/100, Batch 139/1391, Loss: 0.0034604379907250404\n",
      "Epoch 11/100, Batch 278/1391, Loss: 0.0038739352021366358\n",
      "Epoch 11/100, Batch 417/1391, Loss: 0.004036239348351955\n",
      "Epoch 11/100, Batch 556/1391, Loss: 0.00397328520193696\n",
      "Epoch 11/100, Batch 695/1391, Loss: 0.003963307477533817\n",
      "Epoch 11/100, Batch 834/1391, Loss: 0.004276209510862827\n",
      "Epoch 11/100, Batch 973/1391, Loss: 0.003950925078243017\n",
      "Epoch 11/100, Batch 1112/1391, Loss: 0.003739011473953724\n",
      "Epoch 11/100, Batch 1251/1391, Loss: 0.004222269169986248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 11/100 [48:06<6:29:31, 262.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/100, Batch 1390/1391, Loss: 0.003049023449420929\n",
      "Epoch 12/100, Batch 0/1391, Loss: 0.003368351375684142\n",
      "Epoch 12/100, Batch 139/1391, Loss: 0.0034302978310734034\n",
      "Epoch 12/100, Batch 278/1391, Loss: 0.003785313107073307\n",
      "Epoch 12/100, Batch 417/1391, Loss: 0.003938687965273857\n",
      "Epoch 12/100, Batch 556/1391, Loss: 0.003916731104254723\n",
      "Epoch 12/100, Batch 695/1391, Loss: 0.00389113905839622\n",
      "Epoch 12/100, Batch 834/1391, Loss: 0.004186142235994339\n",
      "Epoch 12/100, Batch 973/1391, Loss: 0.003908249083906412\n",
      "Epoch 12/100, Batch 1112/1391, Loss: 0.0036769784055650234\n",
      "Epoch 12/100, Batch 1251/1391, Loss: 0.004078512545675039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 12/100 [52:28<6:25:04, 262.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/100, Batch 1390/1391, Loss: 0.002997560193762183\n",
      "Epoch 13/100, Batch 0/1391, Loss: 0.0033331194426864386\n",
      "Epoch 13/100, Batch 139/1391, Loss: 0.003402861300855875\n",
      "Epoch 13/100, Batch 278/1391, Loss: 0.0037263906560838223\n",
      "Epoch 13/100, Batch 417/1391, Loss: 0.003896048991009593\n",
      "Epoch 13/100, Batch 556/1391, Loss: 0.003865809878334403\n",
      "Epoch 13/100, Batch 695/1391, Loss: 0.003775780089199543\n",
      "Epoch 13/100, Batch 834/1391, Loss: 0.004128935746848583\n",
      "Epoch 13/100, Batch 973/1391, Loss: 0.0038872857112437487\n",
      "Epoch 13/100, Batch 1112/1391, Loss: 0.003653877414762974\n",
      "Epoch 13/100, Batch 1251/1391, Loss: 0.004028670955449343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 13/100 [56:51<6:20:41, 262.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/100, Batch 1390/1391, Loss: 0.0029009757563471794\n",
      "Epoch 14/100, Batch 0/1391, Loss: 0.003278088290244341\n",
      "Epoch 14/100, Batch 139/1391, Loss: 0.003376986365765333\n",
      "Epoch 14/100, Batch 278/1391, Loss: 0.0036892902571707964\n",
      "Epoch 14/100, Batch 417/1391, Loss: 0.003848089836537838\n",
      "Epoch 14/100, Batch 556/1391, Loss: 0.003819767851382494\n",
      "Epoch 14/100, Batch 695/1391, Loss: 0.0037171272560954094\n",
      "Epoch 14/100, Batch 834/1391, Loss: 0.004103981424123049\n",
      "Epoch 14/100, Batch 973/1391, Loss: 0.003864023834466934\n",
      "Epoch 14/100, Batch 1112/1391, Loss: 0.003626142628490925\n",
      "Epoch 14/100, Batch 1251/1391, Loss: 0.003987183328717947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 14/100 [1:01:13<6:16:16, 262.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/100, Batch 1390/1391, Loss: 0.0028689694590866566\n",
      "Epoch 15/100, Batch 0/1391, Loss: 0.003259641584008932\n",
      "Epoch 15/100, Batch 139/1391, Loss: 0.0033505037426948547\n",
      "Epoch 15/100, Batch 278/1391, Loss: 0.0036287393886595964\n",
      "Epoch 15/100, Batch 417/1391, Loss: 0.0037722750566899776\n",
      "Epoch 15/100, Batch 556/1391, Loss: 0.003783648367971182\n",
      "Epoch 15/100, Batch 695/1391, Loss: 0.003671746701002121\n",
      "Epoch 15/100, Batch 834/1391, Loss: 0.004053430166095495\n",
      "Epoch 15/100, Batch 973/1391, Loss: 0.00383047410286963\n",
      "Epoch 15/100, Batch 1112/1391, Loss: 0.0035825553350150585\n",
      "Epoch 15/100, Batch 1251/1391, Loss: 0.003939858637750149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 15/100 [1:05:36<6:11:55, 262.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/100, Batch 1390/1391, Loss: 0.00284550990909338\n",
      "Epoch 16/100, Batch 0/1391, Loss: 0.0032043757382780313\n",
      "Epoch 16/100, Batch 139/1391, Loss: 0.003324377816170454\n",
      "Epoch 16/100, Batch 278/1391, Loss: 0.0035977463703602552\n",
      "Epoch 16/100, Batch 417/1391, Loss: 0.0037078089080750942\n",
      "Epoch 16/100, Batch 556/1391, Loss: 0.0037196732591837645\n",
      "Epoch 16/100, Batch 695/1391, Loss: 0.0036208799574524164\n",
      "Epoch 16/100, Batch 834/1391, Loss: 0.004010818433016539\n",
      "Epoch 16/100, Batch 973/1391, Loss: 0.0038063968531787395\n",
      "Epoch 16/100, Batch 1112/1391, Loss: 0.003527275286614895\n",
      "Epoch 16/100, Batch 1251/1391, Loss: 0.003922350239008665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 16/100 [1:09:59<6:07:36, 262.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/100, Batch 1390/1391, Loss: 0.002798818051815033\n",
      "Epoch 17/100, Batch 0/1391, Loss: 0.0031689044553786516\n",
      "Epoch 17/100, Batch 139/1391, Loss: 0.0032997471280395985\n",
      "Epoch 17/100, Batch 278/1391, Loss: 0.003566885832697153\n",
      "Epoch 17/100, Batch 417/1391, Loss: 0.003676369786262512\n",
      "Epoch 17/100, Batch 556/1391, Loss: 0.003653602907434106\n",
      "Epoch 17/100, Batch 695/1391, Loss: 0.0035631456412374973\n",
      "Epoch 17/100, Batch 834/1391, Loss: 0.003972898703068495\n",
      "Epoch 17/100, Batch 973/1391, Loss: 0.0037647930439561605\n",
      "Epoch 17/100, Batch 1112/1391, Loss: 0.0034953244030475616\n",
      "Epoch 17/100, Batch 1251/1391, Loss: 0.003876172238960862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 17/100 [1:14:21<6:03:15, 262.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/100, Batch 1390/1391, Loss: 0.002753953682258725\n",
      "Epoch 18/100, Batch 0/1391, Loss: 0.00313375610858202\n",
      "Epoch 18/100, Batch 139/1391, Loss: 0.003273540874943137\n",
      "Epoch 18/100, Batch 278/1391, Loss: 0.00353226438164711\n",
      "Epoch 18/100, Batch 417/1391, Loss: 0.003660206450149417\n",
      "Epoch 18/100, Batch 556/1391, Loss: 0.003602575743570924\n",
      "Epoch 18/100, Batch 695/1391, Loss: 0.0035355279687792063\n",
      "Epoch 18/100, Batch 834/1391, Loss: 0.00394072663038969\n",
      "Epoch 18/100, Batch 973/1391, Loss: 0.0037234097253531218\n",
      "Epoch 18/100, Batch 1112/1391, Loss: 0.0034835324622690678\n",
      "Epoch 18/100, Batch 1251/1391, Loss: 0.0038301178719848394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 18/100 [1:18:44<5:58:53, 262.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/100, Batch 1390/1391, Loss: 0.002724003279581666\n",
      "Epoch 19/100, Batch 0/1391, Loss: 0.0030956612899899483\n",
      "Epoch 19/100, Batch 139/1391, Loss: 0.0032647359184920788\n",
      "Epoch 19/100, Batch 278/1391, Loss: 0.0035039838403463364\n",
      "Epoch 19/100, Batch 417/1391, Loss: 0.0036346372216939926\n",
      "Epoch 19/100, Batch 556/1391, Loss: 0.0035827034153044224\n",
      "Epoch 19/100, Batch 695/1391, Loss: 0.0035240361467003822\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training for {num_epochs} epochs started.\")\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, (sample, survey_id, labels, count) in enumerate(train_loader):\n",
    "        samples = [tensor.to(device) for tensor in sample]\n",
    "        survey_id = survey_id.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # with torch.set_grad_enabled(True):\n",
    "        outputs = model(samples)\n",
    "\n",
    "        pos_weight = labels*1.0  # All positive weights are equal to 10\n",
    "        criterion = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % (len(train_loader)//10) == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item()}\")\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        torch.save(model.state_dict(), new_dir / f\"multimodal-epoch-{epoch}.pth\")\n",
    "\n",
    "    scheduler.step()\n",
    "torch.save(model.state_dict(), new_dir / \"multimodal-final.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fa615b",
   "metadata": {},
   "source": [
    "## Produce Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad6677f",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-30T21:25:34.536634Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-05-05T14:42:52.815910Z",
     "iopub.status.busy": "2024-05-05T14:42:52.815177Z",
     "iopub.status.idle": "2024-05-05T14:44:06.291126Z",
     "shell.execute_reply": "2024-05-05T14:44:06.288992Z"
    },
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 73.485486,
     "end_time": "2024-05-05T14:44:06.293057",
     "exception": false,
     "start_time": "2024-05-05T14:42:52.807571",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4716/4716 [00:11<00:00, 419.52it/s]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    surveys = []\n",
    "    top_indices = []\n",
    "    for data, surveyID in tqdm(test_loader, total=len(test_loader)):\n",
    "\n",
    "        data = [tensor.to(device) for tensor in data]\n",
    "\n",
    "        outputs = model(data)\n",
    "        predictions = torch.sigmoid(outputs).cpu().numpy()\n",
    "        predictions = np.squeeze(predictions)\n",
    "        # print(predictions)\n",
    "        prediction = np.argwhere(predictions>=0.25).flatten()\n",
    "        top_indices.append(prediction)\n",
    "        surveys.extend(surveyID.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6632f95d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-05T14:44:06.404548Z",
     "iopub.status.busy": "2024-05-05T14:44:06.404201Z",
     "iopub.status.idle": "2024-05-05T14:44:06.506275Z",
     "shell.execute_reply": "2024-05-05T14:44:06.505581Z"
    },
    "papermill": {
     "duration": 0.159619,
     "end_time": "2024-05-05T14:44:06.508072",
     "exception": false,
     "start_time": "2024-05-05T14:44:06.348453",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_concatenated = [' '.join(map(str, row)) for row in top_indices]\n",
    "\n",
    "pd.DataFrame(\n",
    "    {'surveyId': surveys,\n",
    "     'predictions': data_concatenated,\n",
    "    }).to_csv(new_dir / \"submission.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f553810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of predictions: 79292 with average of 16.81340118744699 predictions per survey ID.\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "for row in top_indices:\n",
    "    total += row.shape[0]\n",
    "\n",
    "print(f\"Total number of predictions: {total} with average of {total/len(top_indices)} predictions per survey ID.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 8171035,
     "sourceId": 64733,
     "sourceType": "competition"
    },
    {
     "isSourceIdPinned": true,
     "modelInstanceId": 36765,
     "sourceId": 43782,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 102.728954,
   "end_time": "2024-05-05T14:44:08.185017",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-05-05T14:42:25.456063",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
